const logger = require('../utils/logger');
const { PrismaClient } = require('@prisma/client');
const azureTTSRestService = require('../services/azureTTSRestService');
const OpenAIService = require('../services/openaiService');
const RealtimeTranscription = require('../services/realtimeTranscription');
// ELIMINADO: EventBasedStateManager y ProductionSafeTimeouts - reemplazados por patr√≥n start/stop simple
const fs = require('fs');

/**
 * Preprocesador de audio para detectar y corregir saturaci√≥n
 */
class AudioPreprocessor {
  constructor() {
    this.gainReduction = 0.7; // Reducir ganancia al 70%
  }
  
  /**
   * Detectar saturaci√≥n temprana en el audio
   */
  detectSaturation(audioData, threshold = 254) {
    const samples = new Uint8Array(audioData);
    const saturatedSamples = samples.filter(s => s >= threshold || s <= 3).length;
    const ratio = saturatedSamples / samples.length;
    
    const stats = {
      max: Math.max(...samples),
      min: Math.min(...samples),
      avg: samples.reduce((a, b) => a + b) / samples.length
    };
    
    // Condiciones m√∫ltiples para detectar saturaci√≥n (umbrales m√°s permisivos)
    const rejectionConditions = [
      stats.max >= 254 && stats.min >= 240,  // Solo si REALMENTE saturado (max Y min altos)
      stats.min >= 230,                      // M√≠nimo extremadamente alto
      stats.avg >= 230,                      // Promedio extremadamente alto
      stats.max - stats.min < 5,             // Rango din√°mico casi nulo
      ratio > 0.3                            // >30% de muestras saturadas
    ];
    
    return {
      isSaturated: rejectionConditions.some(condition => condition),
      ratio: ratio,
      saturatedCount: saturatedSamples,
      stats: stats,
      rejectionReasons: rejectionConditions.map((cond, i) => cond ? i : null).filter(x => x !== null)
    };
  }
  
  /**
   * Aplicar reducci√≥n de ganancia agresiva
   */
  applyGainReduction(audioData) {
    const samples = new Uint8Array(audioData);
    return samples.map(sample => {
      // Convertir de uint8 a int8, aplicar ganancia, volver a uint8
      const intSample = sample - 128;
      const reduced = intSample * this.gainReduction;
      return Math.max(0, Math.min(255, reduced + 128));
    });
  }
  
  /**
   * Normalizaci√≥n agresiva para audio saturado
   */
  normalizeAudio(audioData, targetGain = 0.7) {
    const samples = new Uint8Array(audioData);
    const maxVal = Math.max(...samples.map(s => Math.abs(s - 127)));
    
    if (maxVal === 0) return samples;
    
    // Normalizaci√≥n agresiva para reducir saturaci√≥n
    const scaleFactor = Math.min(targetGain, 80 / maxVal);
    return samples.map(sample => {
      const centered = sample - 127;
      const scaled = centered * scaleFactor;
      return Math.round(Math.max(0, Math.min(255, scaled + 127)));
    });
  }
}

class TwilioStreamHandler {
  constructor(prisma, ttsService) {
    this.prisma = prisma;
    this.ttsService = ttsService; // FIX: Asignar el servicio TTS
    
    // INICIALIZACI√ìN FALTANTE - Servicios cr√≠ticos:
    this.openaiService = new OpenAIService(); // ‚úÖ INICIALIZAR
    this.conversationState = new Map(); // ‚úÖ INICIALIZAR
    this.pendingMarks = new Map(); // ‚úÖ INICIALIZAR - CR√çTICO PARA EVITAR ERRORES

    // Verificar inmediatamente despu√©s de inicializar
    if (!this.pendingMarks || !(this.pendingMarks instanceof Map)) {
      logger.error('üö® CR√çTICO: pendingMarks no se inicializ√≥ correctamente en constructor');
      throw new Error('pendingMarks initialization failed');
    }

    // Mapas para gesti√≥n de estado y audio
    this.activeStreams = new Map();
    this.audioBuffers = new Map();
    this.audioBuffer = new Map(); // FIX: Inicializar audioBuffer para evitar error 'set' undefined
    this.echoBlanking = new Map();
    this.transcriptionActive = new Map();
    this.responseInProgress = new Map();
    this.speechDetection = new Map();
    this.silenceStartTime = new Map();
    this.lastResponseTime = new Map();
    this.energySamples = new Map(); // Para umbrales adaptativos VAD
    this.audioPreprocessor = new AudioPreprocessor();

    // Configurar transcripci√≥n en tiempo real
    this.transcriptionService = new RealtimeTranscription();
    this.fallbackAudio = this.generateFallbackAudio();

    // Voice mapping from user-friendly names to Azure TTS voice identifiers
    // Voz √∫nica para todos los usuarios: Isidora Multiling√ºe (soporte SSML completo)
    this.defaultVoice = 'es-ES-IsidoraMultilingualNeural';
    
    // LOGS DE DIAGN√ìSTICO - Verificar inicializaci√≥n
    logger.info('üîç DIAGN√ìSTICO - Servicios inicializados:');
    logger.info(`üîç - openaiService: ${!!this.openaiService}`);
    logger.info(`üîç - conversationState: ${!!this.conversationState}`);
    logger.info(`üîç - pendingMarks: ${!!this.pendingMarks} (size: ${this.pendingMarks.size})`);
    logger.info(`üîç - transcriptionActive: ${!!this.transcriptionActive}`);
    logger.info(`üîç - transcriptionService: ${!!this.transcriptionService}`);
    
    logger.info('üöÄ TwilioStreamHandler inicializado con patr√≥n Start/Stop simplificado');
  }

  /**
   * Maneja conexi√≥n WebSocket establecida
   */
  handleConnection(ws) {
    logger.info(`üîå NUEVA CONEXI√ìN WEBSOCKET ESTABLECIDA`);
    
    ws.connectionId = `conn_${Date.now()}_${Math.random().toString(36).substr(2, 8)}`;
    logger.info(`üîå Connection ID asignado: ${ws.connectionId}`);
    
    ws.on('message', async (message) => {
      try {
        const data = JSON.parse(message.toString());
        logger.info(`üì® [${ws.connectionId}] Mensaje recibido: ${data.event}`);
        await this.handleMessage(ws, data);
      } catch (error) {
        logger.error(`‚ùå Error procesando mensaje: ${error.message}`);
      }
    });

    ws.on('error', (error) => {
      logger.error(`‚ùå Error en WebSocket ${ws.connectionId}: ${error.message}`);
    });

    ws.on('close', () => {
      logger.info(`üîå Conexi√≥n WebSocket cerrada: ${ws.connectionId}`);
      this.cleanup(ws.connectionId);
    });
  }

  /**
   * Maneja mensajes WebSocket entrantes
   * @param {WebSocket} ws - Conexi√≥n WebSocket
   * @param {Object} data - Datos del mensaje
   */
  handleMessage(ws, data) {
    const { event } = data;
    
    switch (event) {
      case 'connected':
        this.handleConnected(ws, data);
        break;
      case 'start':
        this.handleStart(ws, data);
        break;
      case 'media':
        this.handleMediaEvent(ws, data);
        break;
      case 'mark':
        this.handleMark(data);
        break;
      case 'stop':
        this.handleStop(ws, data);
        break;
      default:
        logger.warn(`‚ö†Ô∏è Evento WebSocket no reconocido: ${event}`);
    }
  }

  /**
   * Maneja evento 'connected' de Twilio
   */
  handleConnected(ws, data) {
    logger.info(`üîå STREAM CONECTADO: ${JSON.stringify(data)}`);
    logger.info(`üîå [${ws.connectionId}] Twilio Stream conectado exitosamente`);

    // IMPORTANTE: El evento 'connected' NO incluye streamSid, por lo que necesitamos usar un ID temporal
    // Usaremos el connectionId como ID temporal hasta que llegue el 'start' con streamSid real
    const tempId = ws.connectionId; // Usar connectionId como ID temporal

    // Obtener datos del cliente usando la informaci√≥n de la llamada
    // Nota: En 'connected', no tenemos streamSid, pero tenemos callSid en el TwiML
    // Sin embargo, para simplicidad, intentaremos obtener el cliente si est√° disponible
    // Si no, lo haremos en 'start' cuando tengamos streamSid
    this.activeStreams.set(tempId, {
      callSid: data.callSid || 'unknown', // Si callSid est√° disponible en 'connected'
      state: 'connected',
      startTime: Date.now(),
      lastActivity: Date.now(),
      tempId: tempId // Marcar como temporal
    });

    logger.info(`üîÑ [${tempId}] Stream registrado con ID temporal (esperando streamSid en 'start')`);
  }

  /**
   * Maneja evento 'mark' de Twilio - CR√çTICO para activar transcripci√≥n
   */
  handleMark(data) {
    const streamSid = data.streamSid;
    const markName = data.mark?.name;
    
    if (!markName) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Mensaje mark sin nombre recibido - IGNORANDO`);
      return;
    }
    
    logger.info(`üéØ [${streamSid}] Marca recibida: ${markName}`);
    logger.info(`üîç [${streamSid}] DEBUG handleMark: markData = ${JSON.stringify(data.mark)}`);
    
    // Verificar si tenemos una acci√≥n pendiente para esta marca
    if (!this.pendingMarks) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] pendingMarks no inicializado - IGNORANDO marca ${markName}`);
      logger.error(`üîç DEBUG: pendingMarks is undefined! Checking constructor initialization.`);
      return;
    }
    
    if (!this.pendingMarks.has(markName)) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Marca ${markName} no esperada (no est√° en pendingMarks) - IGNORANDO. pendingMarks entries: ${Array.from(this.pendingMarks.keys()).join(', ')}`);
      return;
    }
    
    const markData = this.pendingMarks.get(markName);
    logger.info(`üöÄ [${streamSid}] Ejecutando acci√≥n para marca ${markName}: ${markData.action}`);
    logger.info(`üîç [${streamSid}] DEBUG handleMark: pendingMarks has ${this.pendingMarks.size} entries`);
    
    // Ejecutar la acci√≥n correspondiente
    switch (markData.action) {
      case 'activate_transcription':
        logger.info(`üé§ [${streamSid}] ACTIVANDO transcripci√≥n tras saludo (marca: ${markName})`);
        // Activar transcripci√≥n directamente
        this.transcriptionActive.set(streamSid, true);
        const streamData = this.activeStreams.get(streamSid);
        if (streamData) {
          streamData.state = 'listening';
          streamData.greetingCompletedAt = Date.now();
        }
        logger.info(`‚úÖ [${streamSid}] Transcripci√≥n activada tras saludo - usuario puede hablar`);
        break;
      case 'deactivate_echo_blanking':
        logger.info(`‚ö° [${streamSid}] DESACTIVANDO echo blanking tras completar respuesta (marca: ${markName})`);
        this.deactivateEchoBlanking(streamSid);
        this.responseInProgress.delete(streamSid);
        logger.info(`‚úÖ [${streamSid}] Echo blanking desactivado - usuario puede hablar de nuevo`);
        break;
      default:
        logger.warn(`‚ö†Ô∏è [${streamSid}] Acci√≥n desconocida para marca ${markName}: ${markData.action} - IGNORANDO`);
    }
    
    // Limpiar la marca procesada
    this.pendingMarks.delete(markName);
    logger.info(`üßπ [${streamSid}] Marca ${markName} procesada y limpiada`);
  }

  /**
   * Activa la transcripci√≥n despu√©s de que el audio termine (llamado por handleMark)
   */
  activateTranscriptionAfterAudio(streamSid) {
    logger.info(`üîç [${streamSid}] Estado ANTES de activar transcripci√≥n:`);
    logger.info(`üîç [${streamSid}] - echoBlanking activo: ${this.echoBlanking.get(streamSid)?.active}`);
    logger.info(`üîç [${streamSid}] - transcripci√≥n activa: ${this.transcriptionActive.get(streamSid)}`);
    logger.info(`üîç [${streamSid}] - streamData state: ${this.activeStreams.get(streamSid)?.state}`);
    
    // Desactivar echo blanking (esto activa la transcripci√≥n autom√°ticamente)
    this.deactivateEchoBlanking(streamSid);
    
    // ACTIVACI√ìN EXPL√çCITA COMO FALLBACK
    if (!this.transcriptionActive.get(streamSid)) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Transcripci√≥n no se activ√≥ autom√°ticamente - activando manualmente`);
      this.transcriptionActive.set(streamSid, true);
      
      const streamData = this.activeStreams.get(streamSid);
      if (streamData) {
        streamData.state = 'listening';
      }
    }
    
    logger.info(`üîç [${streamSid}] Estado DESPU√âS de activar transcripci√≥n:`);
    logger.info(`üîç [${streamSid}] - echoBlanking activo: ${this.echoBlanking.get(streamSid)?.active}`);
    logger.info(`üîç [${streamSid}] - transcripci√≥n activa: ${this.transcriptionActive.get(streamSid)}`);
    logger.info(`üîç [${streamSid}] - streamData state: ${this.activeStreams.get(streamSid)?.state}`);
    
    logger.info(`‚úÖ [${streamSid}] Transcripci√≥n activada exitosamente - el usuario ya puede hablar`);
  }

  /**
   * Obtener datos del cliente para un stream usando callSid
   * @param {string} streamSid - Stream SID
   * @param {string} callSid - Call SID
   */
  async getClientForStream(streamSid, callSid) {
    try {
      logger.info(`üîç [${streamSid}] Obteniendo cliente para callSid: ${callSid}`);
      
      // Buscar el cliente en la base de datos usando callSid
      // Nota: En producci√≥n, callSid puede no estar directamente en la DB, pero lo intentamos
      const client = await this.prisma.client.findFirst({
        where: {
          // Si tienes una relaci√≥n con llamadas, √∫sala aqu√≠
          // Por ahora, asumimos que callSid no est√° en la DB, as√≠ que usamos un cliente por defecto
          // O implementa la l√≥gica real si tienes callSid en tu esquema
        }
      });
      
      if (client) {
        // Actualizar streamData con el cliente
        const streamData = this.activeStreams.get(streamSid);
        if (streamData) {
          streamData.client = client;
          logger.info(`‚úÖ [${streamSid}] Cliente obtenido: ${client.name || client.id}`);
        }
      } else {
        logger.warn(`‚ö†Ô∏è [${streamSid}] No se encontr√≥ cliente para callSid: ${callSid} - usando cliente por defecto`);
        // Usar cliente por defecto o manejar error
        // Por simplicidad, asumir cliente ID 1 como antes
        const defaultClient = await this.prisma.client.findUnique({
          where: { id: 1 }
        });
        if (defaultClient) {
          const streamData = this.activeStreams.get(streamSid);
          if (streamData) {
            streamData.client = defaultClient;
            logger.info(`‚úÖ [${streamSid}] Cliente por defecto obtenido: ${defaultClient.name}`);
          }
        } else {
          logger.error(`‚ùå [${streamSid}] No se pudo obtener cliente por defecto`);
        }
      }
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error obteniendo cliente: ${error.message}`);
      throw error;
    }
  }

  /**
   * Maneja evento 'start' de Twilio Stream
   */
  handleStart(ws, data) {
    const streamSid = data.start?.streamSid;
    const callSid = data.start?.callSid;

    if (!streamSid) {
      logger.error('‚ùå No se recibi√≥ streamSid en evento start');
      return;
    }

    logger.info(`üéµ [${streamSid}] Stream iniciado para llamada ${callSid}`);

    // Buscar el stream temporal usando connectionId
    const tempId = ws.connectionId;
    const tempStreamData = this.activeStreams.get(tempId);

    if (tempStreamData) {
      // Migrar del ID temporal al streamSid real
      this.activeStreams.set(streamSid, {
        ...tempStreamData,
        streamSid: streamSid,
        callSid: callSid,
        state: 'connected',
        startTime: Date.now(),
        lastActivity: Date.now()
      });

      // Eliminar el registro temporal
      this.activeStreams.delete(tempId);

      logger.info(`üîÑ [${streamSid}] Migrado de ID temporal ${tempId} a streamSid real`);
    } else {
      // Si no hay stream temporal, inicializar directamente
      this.activeStreams.set(streamSid, {
        callSid,
        streamSid,
        state: 'connected',
        startTime: Date.now(),
        lastActivity: Date.now()
      });
    }

    // VERIFICACI√ìN CR√çTICA: Solo enviar saludo si no se ha enviado ya
    const existingStreamData = this.activeStreams.get(streamSid);
    if (existingStreamData?.greetingSent) {
      logger.info(`‚ö†Ô∏è [${streamSid}] Saludo ya enviado (greetingSent=true), omitiendo`);
      return;
    }

    logger.info(`üîç [${streamSid}] greetingSent status: ${existingStreamData?.greetingSent}`);

    // Inicializar sistemas necesarios
    this.initializeSpeechDetection(streamSid);
    this.initializeEchoBlanking(streamSid);

    // Obtener cliente y enviar saludo UNA SOLA VEZ
    this.getClientForStream(streamSid, callSid).then(() => {
      // Verificar de nuevo antes de enviar (doble verificaci√≥n)
      const streamData = this.activeStreams.get(streamSid);
      if (streamData?.greetingSent) {
        logger.info(`‚ö†Ô∏è [${streamSid}] Saludo ya enviado durante getClientForStream (greetingSent=true), omitiendo`);
        return;
      }

      logger.info(`üîç [${streamSid}] Enviando saludo despu√©s de getClientForStream - greetingSent: ${streamData?.greetingSent}`);

      this.sendInitialGreeting(ws, { streamSid, callSid }).catch(error => {
        logger.error(`‚ùå [${streamSid}] Error en saludo inicial: ${error.message}`);
      });
    }).catch(error => {
      logger.error(`‚ùå [${streamSid}] Error obteniendo cliente: ${error.message}`);
    });

    // Inicializar buffers y estados
    this.audioBuffers.set(streamSid, []);
    this.audioBuffer.set(streamSid, []);
    this.transcriptionActive.set(streamSid, false);
    this.responseInProgress.set(streamSid, false);
  }

  /**
{{ ... }}
   * Maneja evento 'stop' de Twilio Stream
   */
  handleStop(ws, data) {
    const streamSid = data.stop?.streamSid;
    
    if (streamSid) {
      logger.info(`üõë [${streamSid}] Stream detenido`);
      this.cleanup(streamSid);
    }
  }

  /**
   * Limpia recursos asociados a un stream
   */
  cleanup(streamSid) {
    this.activeStreams.delete(streamSid);
    this.audioBuffers.delete(streamSid);
    this.transcriptionActive.delete(streamSid);
    this.responseInProgress.delete(streamSid);
    this.silenceStartTime.delete(streamSid);
    this.lastResponseTime.delete(streamSid);
    this.speechDetection.delete(streamSid);
    this.echoBlanking.delete(streamSid);
    this.pendingMarks.delete(streamSid);
    if (this.energySamples) {
      this.energySamples.delete(streamSid);
    }
    
    logger.info(`üßπ [${streamSid}] Recursos limpiados`);
  }

  /**
   * Maps a user-friendly voice name to a valid Azure TTS voice identifier
   * @param {string} voiceId - User-friendly voice name
   * @param {string} language - Language code (e.g., 'es-ES', 'en-US')
   * @returns {string} Valid Azure TTS voice identifier
   */
  mapVoiceToAzure(voiceId, language = 'es-ES') {
    // Siempre usar Isidora Multiling√ºe para todos los usuarios
    logger.info(`üéµ Using Isidora Multilingual voice for all users: ${this.defaultVoice}`);
    return this.defaultVoice;
  }

  /**
   * Humanizar texto con SSML para que Isidora Multiling√ºe suene m√°s natural
   * @param {string} text - Texto a humanizar
   * @param {string} style - Estilo SSML: 'chat', 'empathetic', 'friendly', 'calm'
   * @returns {string} Texto con SSML aplicado (solo contenido interno)
   */
  humanizeTextWithSSML(text, style = 'chat') {
    // Limpiar texto de posibles caracteres problem√°ticos
    const cleanText = text.replace(/[<>&"']/g, (match) => {
      const entities = { '<': '&lt;', '>': '&gt;', '&': '&amp;', '"': '&quot;', "'": '&apos;' };
      return entities[match];
    });

    // Solo devolver el contenido SSML interno (sin <speak> wrapper)
    // El servicio TTS ya agrega el wrapper completo
    // Estilos optimizados para Isidora Multiling√ºe - velocidad 1.1 en todos
    const styleSettings = {
      'chat': { rate: '1.1', pitch: '-2%', volume: '90%', breakTime: '400ms' },
      'empathetic': { rate: '1.1', pitch: '-3%', volume: '85%', breakTime: '500ms' },
      'friendly': { rate: '1.1', pitch: '+1%', volume: '95%', breakTime: '250ms' },
      'calm': { rate: '1.1', pitch: '-4%', volume: '80%', breakTime: '600ms' },
      'cheerful': { rate: '1.1', pitch: '+2%', volume: '95%', breakTime: '300ms' }
    };
    
    const settings = styleSettings[style] || styleSettings['chat'];
    
    const ssmlContent = `
          <mstts:express-as style="${style}">
            <prosody rate="${settings.rate}" pitch="${settings.pitch}" volume="${settings.volume}">
              ${cleanText.replace(/\./g, `.<break time="${settings.breakTime}"/>`)}
            </prosody>
          </mstts:express-as>
    `.trim();

    logger.info(`üé≠ SSML humanizado aplicado: ${ssmlContent.substring(0, 100)}...`);
    return ssmlContent;
  }

  /**
   * Procesar eventos de Twilio Stream - FLUJO LIMPIO
   */
  async processStreamEvent(ws, data) {
    const event = data.event;
    const streamSid = data.streamSid || data.start?.streamSid || 'unknown';
    
    switch (event) {
      case 'connected':
        await this.handleConnected(ws, data);
        break;
      case 'start':
        await this.handleStart(ws, data);
        break;
      case 'media':
        await this.handleMediaEvent(ws, data);
        break;
      case 'mark':
        await this.handleMark(ws, data);
        break;
    }
  }

  /**
   * Stream conectado - SOLO registrar conexi√≥n
   */

  /**
   * Generar saludo inicial - SOLO UNA VEZ POR STREAM
   */
  async sendInitialGreeting(ws, { streamSid, callSid }) {
    logger.info(`üîç [${streamSid}] INICIANDO sendInitialGreeting`);
    const streamData = this.activeStreams.get(streamSid);
    if (!streamData?.client) {
      logger.error(`‚ùå [${streamSid}] Sin configuraci√≥n de cliente`);
      return;
    }

    // Marcar como saludo enviado para evitar duplicados
    streamData.greetingSent = true;
    logger.info(`üîç [${streamSid}] Marcando saludo como enviado`);

    const clientConfigData = await this.prisma.client.findUnique({
      where: { id: parseInt(streamData.client.id) },
      select: {
        callConfig: true
      }
    });

    const greeting = clientConfigData.callConfig?.greeting;
    logger.info(`üîä [${streamSid}] Using greeting from DB: "${greeting}"`);
    
    // Get voice configuration and map to valid Azure voice
    const rawVoiceId = streamData.client.callConfig?.voiceId || 
                      clientConfigData.callConfig?.voiceId || 
                      'isidora';
    const language = streamData.client.callConfig?.language || 
                    clientConfigData.callConfig?.language || 
                    'es-ES';
    
    // DEBUG: Log complete callConfig structure
    logger.info(`üîç [${streamSid}] Complete callConfig from streamData: ${JSON.stringify(streamData.client.callConfig, null, 2)}`);
    logger.info(`üîç [${streamSid}] Complete callConfig from DB: ${JSON.stringify(clientConfigData.callConfig, null, 2)}`);
    
    const voiceId = this.mapVoiceToAzure(rawVoiceId, language);
    
    logger.info(`üéµ [${streamSid}] Raw voice from DB: "${rawVoiceId}"`);
    logger.info(`üéµ [${streamSid}] Mapped Azure voice: "${voiceId}"`);
    logger.info(`üåç [${streamSid}] Language: "${language}"`);
  
    logger.info(`üîä [${streamSid}] Generando saludo: "${greeting?.substring(0, 50)}..."`);

    // Verificar longitud m√≠nima (10 caracteres)
    if (!greeting || greeting.length < 10) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Saludo muy corto o vac√≠o: "${greeting}" - usando fallback`);
      await this.sendExtendedGreeting(ws, streamSid, streamData.client);
      return;
    }

    try {
      // 3. Humanizar el saludo con SSML
      const humanizedGreeting = this.humanizeTextWithSSML(greeting);
      logger.info(`üé≠ [${streamSid}] SSML generado: ${humanizedGreeting.substring(0, 100)}...`);
      
      // 4. Generar audio con Azure TTS usando SSML humanizado con timeout
      logger.info(`üîä [${streamSid}] Iniciando Azure TTS con timeout de 10s...`);
      const ttsPromise = this.ttsService.generateSpeech(
        humanizedGreeting,
        voiceId,
        'raw-8khz-8bit-mono-mulaw'
      );
      
      // Agregar timeout para evitar que Azure TTS se cuelgue en producci√≥n
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error('Azure TTS timeout after 10 seconds')), 10000);
      });
      
      const ttsResult = await Promise.race([ttsPromise, timeoutPromise]);
      logger.info(`üîç [${streamSid}] TTS Result: success=${ttsResult?.success}, audioBuffer length=${ttsResult?.audioBuffer?.length}`);
      
      // ECHO BLANKING: Activar blanking antes de enviar audio del bot
      this.activateEchoBlanking(streamSid);

      if (ttsResult.success) {
        // Save audio to file
        const fileName = `debug_${Date.now()}_${streamSid}.wav`;
        fs.writeFileSync(fileName, ttsResult.audioBuffer);
        logger.info(`üîß [${streamSid}] Audio guardado en ${fileName}`);
        
        logger.info(`üîç [${streamSid}] ANTES de sendRawMulawToTwilio`);
        
        // Calcular duraci√≥n aproximada del audio para timing correcto
        const audioLengthMs = Math.ceil((ttsResult.audioBuffer.length / 8000) * 1000); // 8kHz mulaw
        logger.info(`üîç [${streamSid}] Audio length: ${ttsResult.audioBuffer.length} bytes = ~${audioLengthMs}ms`);
        
        // Usar sistema de marcas para activar transcripci√≥n despu√©s del saludo
        const markId = `greeting_end_${Date.now()}`;
        logger.info(`üéØ [${streamSid}] Enviando saludo con marca: ${markId}`);
        
        // Registrar que esperamos esta marca para activar transcripci√≥n
        this.pendingMarks = this.pendingMarks || new Map();
        this.pendingMarks.set(markId, {
          streamSid: streamSid,
          action: 'activate_transcription',
          timestamp: Date.now()
        });
        
        // Enviar audio con marca al final
        await this.sendRawMulawToTwilioWithMark(ws, ttsResult.audioBuffer, streamSid, markId);
        logger.info(`‚úÖ [${streamSid}] Audio del saludo enviado con marca ${markId}`);
      } else {
        logger.error(`‚ùå [${streamSid}] TTS fall√≥: ${ttsResult?.error || 'Unknown error'}`);
        throw new Error('TTS failed');
      }
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error TTS: ${error.message}`);
      
      // 4. Usar fallback si TTS falla
      logger.warn(`‚ö†Ô∏è [${streamSid}] Usando audio de fallback`);
      
      // Activar echo blanking durante fallback
      this.activateEchoBlanking(streamSid);
      
      // Usar sistema de marcas para activar transcripci√≥n despu√©s del fallback
      const markId = `fallback_end_${Date.now()}`;
      logger.info(`üéØ [${streamSid}] Enviando fallback con marca: ${markId}`);
      
      // Registrar que esperamos esta marca para activar transcripci√≥n
      this.pendingMarks = this.pendingMarks || new Map();
      this.pendingMarks.set(markId, {
        streamSid: streamSid,
        action: 'activate_transcription',
        timestamp: Date.now()
      });
      
      // Enviar fallback con marca al final
      await this.sendRawMulawToTwilioWithMark(ws, this.fallbackAudio, streamSid, markId);
      logger.info(`‚úÖ [${streamSid}] Audio fallback del saludo enviado con marca ${markId}`);
    }
    logger.info(`üîç [${streamSid}] FINALIZANDO sendInitialGreeting`);
  }

  async sendExtendedGreeting(ws, streamSid, clientConfigData) {
    const fallbackGreeting = "Gracias por llamar. Estamos conect√°ndote con un asistente. Por favor, espera un momento.";
    
    // Get voice configuration and map to valid Azure voice
    const rawVoiceId = clientConfigData.callConfig?.voiceId || 
                      'isidora';
    const language = clientConfigData.callConfig?.language || 
                    'es-ES';
    
    logger.info(`üîä [${streamSid}] Generando saludo extendido de fallback`);
    logger.info(`üéµ [${streamSid}] Raw voice: "${rawVoiceId}" ‚Üí Mapped: "${this.mapVoiceToAzure(rawVoiceId, language)}"`);
  
    logger.info(`üîä [${streamSid}] Generando saludo extendido de fallback con voz: ${this.mapVoiceToAzure(rawVoiceId, language)}`);
  
    // Humanizar el saludo de fallback con SSML
    const humanizedFallback = this.humanizeTextWithSSML(fallbackGreeting);
    
    try {
      logger.info(`üîä [${streamSid}] Iniciando Azure TTS para saludo extendido con timeout de 10s...`);
      const ttsPromise = this.ttsService.generateSpeech(
        humanizedFallback,
        this.mapVoiceToAzure(rawVoiceId, language),
        'raw-8khz-8bit-mono-mulaw'
      );
      
      // Agregar timeout para evitar que Azure TTS se cuelgue en producci√≥n
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error('Azure TTS timeout after 10 seconds')), 10000);
      });
      
      const ttsResult = await Promise.race([ttsPromise, timeoutPromise]);
      
      // ECHO BLANKING: Activar blanking antes de enviar audio del bot
      this.activateEchoBlanking(streamSid);
    
      if (ttsResult.success) {
        // Save audio to file
        const fileName = `debug_${Date.now()}_${streamSid}.wav`;
        fs.writeFileSync(fileName, ttsResult.audioBuffer);
        logger.info(`üîß [${streamSid}] Audio guardado en ${fileName}`);
        
        // Calcular duraci√≥n aproximada del audio para timing correcto
        const audioLengthMs = Math.ceil((ttsResult.audioBuffer.length / 8000) * 1000); // 8kHz mulaw
        logger.info(`üîç [${streamSid}] Audio length: ${ttsResult.audioBuffer.length} bytes = ~${audioLengthMs}ms`);
        
        // Usar sistema de marcas para activar transcripci√≥n
        const markId = `extended_greeting_end_${Date.now()}`;
        logger.info(`üéØ [${streamSid}] Enviando saludo extendido con marca: ${markId}`);
        
        // Registrar que esperamos esta marca para activar transcripci√≥n
        this.pendingMarks = this.pendingMarks || new Map();
        this.pendingMarks.set(markId, {
          streamSid: streamSid,
          action: 'activate_transcription',
          timestamp: Date.now()
        });
        
        // Enviar audio con marca al final
        await this.sendRawMulawToTwilioWithMark(ws, ttsResult.audioBuffer, streamSid, markId);
        logger.info(`‚úÖ [${streamSid}] Saludo extendido enviado con marca ${markId}`);
      }
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error en saludo extendido: ${error.message}`);
      
      // Usar audio de fallback si TTS falla
      logger.warn(`‚ö†Ô∏è [${streamSid}] Usando audio de fallback para saludo extendido`);
      
      // Activar echo blanking durante fallback
      this.activateEchoBlanking(streamSid);
      
      // Usar sistema de marcas para activar transcripci√≥n despu√©s del fallback extendido
      const markId = `extended_fallback_end_${Date.now()}`;
      logger.info(`üéØ [${streamSid}] Enviando fallback extendido con marca: ${markId}`);
      
      // Registrar que esperamos esta marca para activar transcripci√≥n
      this.pendingMarks = this.pendingMarks || new Map();
      this.pendingMarks.set(markId, {
        streamSid: streamSid,
        action: 'activate_transcription',
        timestamp: Date.now()
      });
      
      // Enviar fallback con marca al final
      await this.sendRawMulawToTwilioWithMark(ws, this.fallbackAudio, streamSid, markId);
      logger.info(`‚úÖ [${streamSid}] Fallback del saludo extendido enviado con marca ${markId}`);
    }
  }

  async sendRawMulawToTwilio(ws, mulawBuffer, streamSid) {
    const chunkSize = 160;
    let offset = 0;
    
    while (offset < mulawBuffer.length) {
      const chunk = mulawBuffer.subarray(offset, offset + chunkSize);
      const base64Chunk = chunk.toString('base64');
      
      ws.send(JSON.stringify({
        event: 'media',
        streamSid: streamSid,
        media: { payload: base64Chunk }
      }));
      
      offset += chunkSize;
    }
    
    logger.info(`‚úÖ [${streamSid}] Audio enviado: ${Math.ceil(mulawBuffer.length / chunkSize)} chunks`);
  }

  /**
   * Enviar audio con marca para detectar cuando termina la reproducci√≥n
   */
  async sendRawMulawToTwilioWithMark(ws, mulawBuffer, streamSid, markId) {
    logger.info(`üîä [${streamSid}] Enviando audio con marca: ${markId}`);
    logger.info(`üîä Tama√±o del buffer de audio: ${mulawBuffer.length} bytes`);
    
    // Ensure minimum audio length (1 second = 8000 bytes)
    if (mulawBuffer.length < 8000) {
      const padding = Buffer.alloc(8000 - mulawBuffer.length, 0xFF);
      mulawBuffer = Buffer.concat([mulawBuffer, padding]);
      logger.info(`üîä [${streamSid}] A√±adido padding de audio: ${padding.length} bytes`);
    }
    
    const chunkSize = 160;
    let offset = 0;
    let chunkCount = 0;
    
    logger.info(`üéµ [${streamSid}] Iniciando transmisi√≥n de audio con marca (${mulawBuffer.length} bytes)`);
    
    // Enviar todos los chunks de audio
    while (offset < mulawBuffer.length) {
      const chunk = mulawBuffer.subarray(offset, offset + chunkSize);
      const base64Chunk = chunk.toString('base64');
      
      ws.send(JSON.stringify({
        event: 'media',
        streamSid: streamSid,
        media: { payload: base64Chunk }
      }));
      
      chunkCount++;
      offset += chunkSize;
    }
    
    // CR√çTICO: Enviar marca DESPU√âS del audio para detectar cuando termina
    ws.send(JSON.stringify({
      event: 'mark',
      streamSid: streamSid,
      mark: { name: markId }
    }));
    
    logger.info(`‚úÖ [${streamSid}] Audio enviado (${chunkCount} chunks) + marca ${markId}`);
  }

  generateFallbackAudio() {
    const buffer = Buffer.alloc(160, 0xff); // Silencio digital
    return buffer;
  }

  /**
   * Inicializar sistema de detecci√≥n de habla para un stream
   */
  initializeSpeechDetection(streamSid) {
    logger.info(`üé§ [${streamSid}] Inicializando detecci√≥n de voz...`);
    
    // CONFIGURACI√ìN VAD OPTIMIZADA PARA TWILIO
    // Basado en sistemas probados: OpenAI Whisper, Silero VAD, WebRTC
    const config = {
      isActive: false,
      silenceCount: 0,
      speechCount: 0,
      lastActivity: Date.now(),
      
      // UMBRALES OPTIMIZADOS PARA Œº-LAW 8kHz
      energyThreshold: 5, // Bajado de 10 a 5 para mayor sensibilidad
      adaptiveThreshold: 5, // Bajado de 10 a 5
      
      // CONTEOS EST√ÅNDAR PARA VAD
      maxSilenceDuration: 4, // 4 chunks = ~320ms de silencio para procesar
      minSpeechDuration: 2, // 2 chunks = ~160ms m√≠nimo de habla
      
      // TIMERS EST√ÅNDAR
      hangoverDuration: 500, // 500ms hangover despu√©s de habla
      hangoverTimer: 0,
      
      // ECHO BLANKING
      echoBlanking: false,
      echoBlankingUntil: 0,
      echoBlankingDuration: 500,
      
      // HISTORIAL PARA UMBRAL ADAPTATIVO
      energyHistory: []
    };
    
    this.speechDetection.set(streamSid, config);
    
    // Verificar que se guard√≥ correctamente
    const verification = this.speechDetection.get(streamSid);
    if (verification) {
      logger.info(`‚úÖ [${streamSid}] Speech detection initialized successfully`);
      logger.info(`üîç [${streamSid}] Config keys: ${Object.keys(verification).join(', ')}`);
    } else {
      logger.error(`‚ùå [${streamSid}] Failed to initialize speech detection`);
    }
  }

  /**
   * Inicializar sistema de echo blanking para un stream
   */
  initializeEchoBlanking(streamSid) {
    this.echoBlanking.set(streamSid, {
      active: false,
      endTime: 0,
      lastActivation: 0
    });
    logger.info(`üîá [${streamSid}] Echo blanking inicializado`);
  }

  /**
   * Activar Echo Blanking cuando el bot va a hablar
   */
  activateEchoBlanking(streamSid) {
    const echoBlanking = this.echoBlanking.get(streamSid);
    if (echoBlanking) {
      const now = Date.now();
      const duration = 2000; // 2 seconds echo blanking duration
      echoBlanking.active = true;
      echoBlanking.endTime = now + duration;
      logger.info(`üîá [${streamSid}] Echo Blanking ACTIVADO por ${duration}ms`);
    }
  }

  /**
   * Desactivar Echo Blanking manualmente
   */
  deactivateEchoBlanking(streamSid) {
    logger.info(`üîç [${streamSid}] INICIANDO deactivateEchoBlanking()`);
    
    const echoBlanking = this.echoBlanking.get(streamSid);
    logger.info(`üîç [${streamSid}] echoBlanking obtenido:`, echoBlanking);
    
    if (!echoBlanking) {
      logger.error(`‚ùå [${streamSid}] NO HAY echoBlanking en el mapa`);
      return;
    }
    
    logger.info(`üîç [${streamSid}] echoBlanking.active = ${echoBlanking.active}`);
    
    if (echoBlanking && echoBlanking.active) {
      logger.info(`üîç [${streamSid}] Desactivando echo blanking...`);
      echoBlanking.active = false;
      echoBlanking.endTime = 0;
      logger.info(`üîá [${streamSid}] Echo Blanking DESACTIVADO`);
      
      // Activar transcripci√≥n autom√°ticamente cuando se desactiva echo blanking
      const streamData = this.activeStreams.get(streamSid);
      logger.info(`üîç [${streamSid}] streamData obtenido:`, streamData ? 'EXISTS' : 'NULL');
      
      const currentTranscriptionState = this.transcriptionActive.get(streamSid);
      logger.info(`üîç [${streamSid}] Estado actual transcripci√≥n: ${currentTranscriptionState}`);
      
      if (streamData && !this.transcriptionActive.get(streamSid)) {
        logger.info(`üîç [${streamSid}] ACTIVANDO TRANSCRIPCI√ìN...`);
        this.transcriptionActive.set(streamSid, true);
        streamData.state = 'listening';
        streamData.greetingCompletedAt = Date.now();
        
        // Verificar que se activ√≥ correctamente
        const newTranscriptionState = this.transcriptionActive.get(streamSid);
        logger.info(`üöÄ [${streamSid}] Transcripci√≥n activada! Nuevo estado: ${newTranscriptionState}`);
        logger.info(`üöÄ [${streamSid}] streamData.state = ${streamData.state}`);
      } else {
        logger.warn(`‚ö†Ô∏è [${streamSid}] NO se activ√≥ transcripci√≥n:`);
        logger.warn(`‚ö†Ô∏è [${streamSid}] - streamData existe: ${!!streamData}`);
        logger.warn(`‚ö†Ô∏è [${streamSid}] - transcripci√≥n ya activa: ${this.transcriptionActive.get(streamSid)}`);
      }
    } else {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Echo blanking NO estaba activo o no existe`);
      logger.warn(`‚ö†Ô∏è [${streamSid}] - echoBlanking existe: ${!!echoBlanking}`);
      logger.warn(`‚ö†Ô∏è [${streamSid}] - echoBlanking.active: ${echoBlanking?.active}`);
    }
    
    logger.info(`üîç [${streamSid}] FINALIZANDO deactivateEchoBlanking()`);
  }

  /**
   * Verificar si Echo Blanking est√° activo
   */
  isEchoBlankingActive(streamSid) {
    const echoBlanking = this.echoBlanking.get(streamSid);
    if (!echoBlanking) return false;
    
    const now = Date.now();
    if (echoBlanking.active && now > echoBlanking.endTime) {
      echoBlanking.active = false;
      logger.info(`üîä [${streamSid}] Echo Blanking DESACTIVADO`);
    }
    
    return echoBlanking.active;
  }

  /**
   * Decodificar byte Œº-law a valor PCM lineal de 16-bit
   * @param {number} mulawByte - Byte Œº-law (0-255)
   * @returns {number} Valor PCM lineal (-32768 a 32767)
   */
  decodeMulaw(mulawByte) {
    // Œº-law decoding algorithm
    mulawByte = ~mulawByte & 0xFF; // Invertir bits

    let sign = (mulawByte & 0x80) ? -1 : 1;
    let exponent = (mulawByte >> 4) & 0x07;
    let mantissa = mulawByte & 0x0F;

    let value = mantissa << 4; // Base value

    // Agregar bias basado en exponent
    switch (exponent) {
      case 0: value += 0; break;
      case 1: value += 16; break;
      case 2: value += 32; break;
      case 3: value += 64; break;
      case 4: value += 128; break;
      case 5: value += 256; break;
      case 6: value += 512; break;
      case 7: value += 1024; break;
    }

    // Ajustar para rango completo
    if (exponent > 1) {
      value += 33; // Bias para Œº-law
    }

    return sign * value * 4; // Escalar a 16-bit range
  }

  /**
   * Detectar actividad de voz en tiempo real usando VAD (Voice Activity Detection)
   * Ahora con correcta decodificaci√≥n Œº-law
   */
  detectVoiceActivity(audioChunk, streamSid) {
    const mulawBytes = new Uint8Array(audioChunk);

    // üîß CRITICAL FIX: Validar datos de entrada
    if (!mulawBytes || mulawBytes.length === 0) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] VAD: Datos Œº-law inv√°lidos o vac√≠os`);
      return { shouldProcess: false, isActive: false, energy: '0.0', threshold: '5.0' };
    }

    let energy = 0;
    let validSamples = 0;

    try {
      // üîß CRITICAL FIX: Decodificar Œº-law a PCM lineal correctamente
      for (let i = 0; i < mulawBytes.length; i++) {
        const mulawByte = mulawBytes[i];

        // Validar que sea un byte v√°lido (0-255)
        if (typeof mulawByte !== 'number' || mulawByte < 0 || mulawByte > 255) {
          logger.warn(`‚ö†Ô∏è [${streamSid}] VAD: Byte Œº-law inv√°lido en posici√≥n ${i}: ${mulawByte}`);
          continue;
        }

        // Decodificar Œº-law a PCM lineal
        const pcmValue = this.decodeMulaw(mulawByte);

        // Validar resultado de decodificaci√≥n
        if (isNaN(pcmValue) || !isFinite(pcmValue)) {
          logger.warn(`‚ö†Ô∏è [${streamSid}] VAD: Valor PCM inv√°lido de byte ${mulawByte}: ${pcmValue}`);
          continue;
        }

        // Calcular energ√≠a del valor PCM (normalizar a 0-1)
        const normalizedValue = Math.abs(pcmValue) / 32768.0;
        energy += normalizedValue * normalizedValue;
        validSamples++;
      }

      // Calcular energ√≠a promedio
      if (validSamples > 0) {
        energy = Math.sqrt(energy / validSamples);
      } else {
        energy = 0;
      }

      // Validar resultado final de energ√≠a
      if (isNaN(energy) || !isFinite(energy)) {
        logger.warn(`‚ö†Ô∏è [${streamSid}] VAD: Energ√≠a final inv√°lida: ${energy}, usando 0`);
        energy = 0;
      }
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] VAD: Error calculando energ√≠a: ${error.message}`);
      energy = 0;
    }

    const detection = this.speechDetection.get(streamSid);

    // üîç DEBUG: Verificar estado del detection
    logger.info(`üîç [${streamSid}] VAD Debug - detection exists: ${!!detection}`);
    if (detection) {
      logger.info(`üîç [${streamSid}] VAD Debug - adaptiveThreshold: ${detection.adaptiveThreshold}, type: ${typeof detection.adaptiveThreshold}`);
      logger.info(`üîç [${streamSid}] VAD Debug - energyThreshold: ${detection.energyThreshold}, type: ${typeof detection.energyThreshold}`);
      logger.info(`üîç [${streamSid}] VAD Debug - valid samples: ${validSamples}/${mulawBytes.length}`);
    }

    if (!detection) {
      logger.error(`‚ùå [${streamSid}] VAD: No hay configuraci√≥n speechDetection`);
      return { shouldProcess: false, isActive: false, energy: energy.toFixed(1), threshold: '5.0' };
    }

    // üîß CRITICAL FIX: Inicializar adaptiveThreshold si es undefined
    if (typeof detection.adaptiveThreshold !== 'number' || isNaN(detection.adaptiveThreshold)) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] VAD: adaptiveThreshold inv√°lido (${detection.adaptiveThreshold}), inicializando...`);

      if (typeof detection.energyThreshold === 'number' && !isNaN(detection.energyThreshold)) {
        detection.adaptiveThreshold = detection.energyThreshold;
        logger.info(`‚úÖ [${streamSid}] VAD: Usando energyThreshold como fallback: ${detection.adaptiveThreshold}`);
      } else {
        detection.adaptiveThreshold = 0.1; // Valor optimizado para Œº-law normalizado (0-1)
        logger.info(`‚úÖ [${streamSid}] VAD: Usando valor por defecto: ${detection.adaptiveThreshold}`);
      }
    }

    const isActive = energy > detection.adaptiveThreshold;

    logger.info(`üé§ [${streamSid}] VAD: energy=${energy.toFixed(3)}, threshold=${detection.adaptiveThreshold.toFixed(3)}, isActive=${isActive}, samples=${validSamples}`);

    // Actualizar contadores
    if (isActive) {
      detection.speechCount++;
      detection.silenceCount = 0;
      detection.isActive = true;
    } else {
      detection.silenceCount++;
      detection.speechCount = 0;
      if (detection.silenceCount > 5) {
        detection.isActive = false;
      }
    }

    detection.lastActivity = Date.now();

    // üîß CRITICAL FIX: Adaptar threshold din√°micamente
    if (detection.energyHistory && detection.energyHistory.length > 0) {
      const avgEnergy = detection.energyHistory.reduce((a, b) => a + b, 0) / detection.energyHistory.length;
      if (!isNaN(avgEnergy) && avgEnergy > 0) {
        // Ajuste gradual del threshold basado en el historial
        detection.adaptiveThreshold = detection.adaptiveThreshold * 0.9 + avgEnergy * 0.1;
      }
    }

    // Mantener historial de energ√≠a para adaptaci√≥n
    if (!detection.energyHistory) detection.energyHistory = [];
    detection.energyHistory.push(energy);
    if (detection.energyHistory.length > 50) {
      detection.energyHistory.shift(); // Mantener solo los √∫ltimos 50 valores
    }

    return {
      shouldProcess: isActive,
      isActive: detection.isActive,
      energy: energy.toFixed(3),
      threshold: detection.adaptiveThreshold.toFixed(3)
    };
  }

  /**
   * Analizar calidad y caracter√≠sticas del buffer de audio
   */

  /**
   * Manejar eventos de media (audio del caller)
   */
  async handleMediaEvent(ws, data) {
    const { streamSid } = data;
    const streamData = this.activeStreams.get(streamSid);
    
    if (!streamData || streamData.state !== 'listening') {
      return;
    }

    const payload = data.media?.payload;
    if (!payload) return;

    const rawAudioChunk = Buffer.from(payload, 'base64');
    const normalizedAudio = this.audioPreprocessor.normalizeAudio(rawAudioChunk, 0.7);
    
    const streamAudioBuffer = this.audioBuffers.get(streamSid) || [];
    streamAudioBuffer.push(normalizedAudio);
    this.audioBuffers.set(streamSid, streamAudioBuffer);
    
    // Log detallado para debug
    logger.info(`üéôÔ∏è [${streamSid}] Chunk de audio recibido: ${normalizedAudio.length} bytes, buffer size: ${streamAudioBuffer.length}`);
    
    const vadResult = this.detectVoiceActivity(streamSid, normalizedAudio);
    
    logger.info(`üîç [${streamSid}] VAD Result: shouldProcess=${vadResult.shouldProcess}, isActive=${vadResult.isActive}, energy=${vadResult.energy}, threshold=${vadResult.threshold}`);
    
    if (vadResult.shouldProcess) {
      const collectedAudio = this.audioBuffers.get(streamSid);
      this.audioBuffers.set(streamSid, []);
      if (collectedAudio && collectedAudio.length > 0) {
        logger.info(`üöÄ [${streamSid}] Procesando ${collectedAudio.length} chunks de audio acumulado`);
        this.processCollectedAudio(ws, streamSid, collectedAudio);
      }
    }
  }

  /**
   * NUEVO: Procesar audio acumulado cuando se detecta fin de turno
   */
  async processCollectedAudio(ws, streamSid, collectedAudio) {
    const streamData = this.activeStreams.get(streamSid);
    logger.info(`üöÄ [${streamSid}] INICIANDO processCollectedAudio con ${collectedAudio.length} chunks`);
    
    if (!collectedAudio || collectedAudio.length === 0) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] No hay audio acumulado para procesar`);
      return;
    }

    // Combinar chunks en un solo buffer
    const combinedBuffer = Buffer.concat(collectedAudio);
    logger.info(`üîß [${streamSid}] Audio combinado: ${combinedBuffer.length} bytes`);

    // Verificar si el audio tiene contenido (no es silencio)
    const silentBytes = combinedBuffer.filter(byte => byte === 0xFF).length;
    const totalBytes = combinedBuffer.length;
    const silencePercentage = (silentBytes / totalBytes) * 100;
    logger.info(`üîá [${streamSid}] Silencio: ${silencePercentage.toFixed(1)}% (${silentBytes}/${totalBytes} bytes)`);

    if (silencePercentage > 95) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Audio es casi silencio (${silencePercentage.toFixed(1)}%), omitiendo`);
      return;
    }

    // Procesar audio con transcripci√≥n
    try {
      logger.info(`üé§ [${streamSid}] Enviando audio a transcripci√≥n (${combinedBuffer.length} bytes)`);
      const transcriptionResult = await this.transcriptionService.transcribeAudio(combinedBuffer);
      
      logger.info(`üìù [${streamSid}] Transcripci√≥n result: ${JSON.stringify(transcriptionResult)}`);
      
      if (transcriptionResult.success && transcriptionResult.text) {
        logger.info(`üí¨ [${streamSid}] Texto transcrito: "${transcriptionResult.text}"`);
        
        // Generar respuesta con OpenAI
        logger.info(`ü§ñ [${streamSid}] Enviando a OpenAI para respuesta`);
        const openaiResponse = await this.openaiService.generateResponse(transcriptionResult.text, streamData);
        
        if (openaiResponse.success) {
          logger.info(`‚úÖ [${streamSid}] Respuesta de OpenAI: "${openaiResponse.response.substring(0, 50)}..."`);
          
          // Generar TTS con la respuesta
          logger.info(`üîä [${streamSid}] Generando TTS para respuesta`);
          const ttsResult = await this.ttsService.generateSpeech(openaiResponse.response, 'es-ES-IsidoraMultilingualNeural', 'raw-8khz-8bit-mono-mulaw');
          
          if (ttsResult.success) {
            logger.info(`‚úÖ [${streamSid}] TTS generado, enviando audio`);
            
            // Activar echo blanking
            this.activateEchoBlanking(streamSid);
            
            // Enviar audio de respuesta
            await this.sendRawMulawToTwilio(ws, ttsResult.audioBuffer, streamSid);
            
            // Desactivar echo blanking despu√©s
            setTimeout(() => {
              this.deactivateEchoBlanking(streamSid);
            }, Math.ceil((ttsResult.audioBuffer.length / 8000) * 1000) + 500);
          } else {
            logger.error(`‚ùå [${streamSid}] Error en TTS de respuesta: ${ttsResult.error}`);
          }
        } else {
          logger.error(`‚ùå [${streamSid}] Error en OpenAI: ${openaiResponse.error}`);
        }
      } else {
        logger.warn(`‚ö†Ô∏è [${streamSid}] Transcripci√≥n fallida o sin texto: ${transcriptionResult.error}`);
      }
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error procesando audio: ${error.message}`);
    }
    logger.info(`üîö [${streamSid}] FINALIZANDO processCollectedAudio`);
  }
}

module.exports = TwilioStreamHandler;
