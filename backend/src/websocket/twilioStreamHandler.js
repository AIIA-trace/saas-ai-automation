const logger = require('../utils/logger');
const { PrismaClient } = require('@prisma/client');
const azureTTSRestService = require('../services/azureTTSRestService');
const OpenAIRealtimeService = require('../services/openaiRealtimeService');
const callerMemoryService = require('../services/callerMemoryService');
const fs = require('fs');

class TwilioStreamHandler {
  constructor(prisma, ttsService) {
    this.prisma = prisma;
    this.ttsService = ttsService; // FIX: Asignar el servicio TTS
    
    // NUEVO: Servicio OpenAI Realtime (reemplaza sistema conversacional complejo)
    this.openaiRealtimeService = new OpenAIRealtimeService();
    
    // CR√çTICO PARA SISTEMA DE MARCAS: Conservar para saludo inicial
    this.pendingMarks = new Map(); // ‚úÖ CR√çTICO PARA EVITAR ERRORES

    // Verificar inmediatamente despu√©s de inicializar
    if (!this.pendingMarks || !(this.pendingMarks instanceof Map)) {
      logger.error('üö® CR√çTICO: pendingMarks no se inicializ√≥ correctamente en constructor');
      throw new Error('pendingMarks initialization failed');
    }

    // Mapas para gesti√≥n de estado y audio
    this.activeStreams = new Map();
    this.audioBuffers = new Map();
    this.audioBuffer = new Map(); // FIX: Inicializar audioBuffer para evitar error 'set' undefined
    this.echoBlanking = new Map();
    this.transcriptionActive = new Map();
    this.responseInProgress = new Map();
    // REMOVIDO: speechDetection (VAD obsoleto - ahora usa OpenAI server VAD)
    this.silenceStartTime = new Map();
    this.lastResponseTime = new Map();

    // NUEVO: Set para trackear streamSids ya procesados - evita duplicados
    this.processedStreamSids = new Set();

    // NUEVO: Cache de clientes para evitar consultas duplicadas
    this.clientCache = new Map();

    this.fallbackAudio = this.generateFallbackAudio();

    // Voice mapping from user-friendly names to Azure TTS voice identifiers
    // Voz √∫nica para todos los usuarios: Isidora Multiling√ºe (soporte SSML completo)
    this.defaultVoice = 'es-ES-IsidoraMultilingualNeural';
    
    // üìä M√âTRICAS PARA MONITOREO PRODUCCI√ìN
    this.metrics = {
      totalCalls: 0,
      activeCalls: 0,
      successfulCalls: 0,
      failedCalls: 0,
      averageCallDuration: 0,
      openaiFailures: 0,
      azureFailures: 0,
      lastReset: Date.now()
    };

    // Configurar event listeners para OpenAI Realtime Service
    this.openaiRealtimeService.on('audioResponse', (data) => {
      this.handleOpenAIAudioResponse(data);
    });

    // NUEVOS EVENT LISTENERS del c√≥digo oficial
    this.openaiRealtimeService.on('clearAudio', (data) => {
      this.handleClearAudio(data);
    });

    this.openaiRealtimeService.on('sendMark', (data) => {
      this.handleSendMark(data);
    });

    // ‚úÖ NUEVO FLUJO SIMPLE: Texto de OpenAI ‚Üí Azure TTS (como saludo inicial)
    this.openaiRealtimeService.on('processTextWithAzure', (data) => {
      this.handleProcessTextWithAzure(data);
    });

    // üöÄ AUDIO NATIVO: Audio directo de OpenAI ‚Üí Twilio (sin Azure TTS)
    this.openaiRealtimeService.on('audioFromOpenAI', (data) => {
      this.handleAudioFromOpenAI(data);
    });

    // LOGS DE DIAGN√ìSTICO - Verificar inicializaci√≥n
    logger.info('üîç DIAGN√ìSTICO - Servicios inicializados:');
    logger.info(`üîç - openaiRealtimeService: ${!!this.openaiRealtimeService}`);
    logger.info(`üîç - pendingMarks: ${!!this.pendingMarks} (size: ${this.pendingMarks.size})`);
    logger.info(`üîç - transcriptionActive: ${!!this.transcriptionActive}`);

    logger.info('üöÄ TwilioStreamHandler inicializado con OpenAI Realtime API');
  }

  /**
   * Manejar respuesta de audio desde OpenAI Realtime Service
   * @param {Object} data - Datos del audio response {streamSid, audioBuffer}
   */
  async handleOpenAIAudioResponse(data) {
    const { streamSid, audioBuffer } = data;
    const streamData = this.activeStreams.get(streamSid);
    
    if (!streamData || !streamData.twilioWs) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] No hay conexi√≥n Twilio activa para enviar respuesta OpenAI`);
      return;
    }

    logger.info(`ü§ñ [${streamSid}] Enviando respuesta de audio OpenAI a Twilio (${audioBuffer.length} bytes)`);

    try {
      // Activar echo blanking durante respuesta del bot
      this.activateEchoBlanking(streamSid);
      
      // üîá DESACTIVAR VAD de OpenAI mientras el bot habla
      if (streamData.openAiService) {
        await streamData.openAiService.disableVAD(streamSid);
      }
      
      // Enviar audio de OpenAI a Twilio usando el sistema de marcas
      const markId = `openai_response_${Date.now()}`;
      this.pendingMarks.set(markId, {
        streamSid: streamSid,
        action: 'deactivate_echo_blanking',
        timestamp: Date.now()
      });
      
      await this.sendRawMulawToTwilioWithMark(streamData.twilioWs, audioBuffer, streamSid, markId);
      logger.info(`‚úÖ [${streamSid}] Respuesta OpenAI enviada con marca ${markId}`);
      
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error enviando respuesta OpenAI: ${error.message}`);
      // Desactivar echo blanking en caso de error
      this.deactivateEchoBlanking(streamSid);
      
      // üé§ REACTIVAR VAD en caso de error
      const streamData = this.activeStreams.get(streamSid);
      if (streamData?.openAiService) {
        await streamData.openAiService.enableVAD(streamSid);
      }
    }
  }

  /**
   * Inicializar conexi√≥n OpenAI Realtime despu√©s del saludo inicial
   * @param {string} streamSid - ID del stream
   */
  async initializeOpenAIRealtimeConnection(streamSid) {
    const streamData = this.activeStreams.get(streamSid);
    if (!streamData) {
      logger.error(`‚ùå [${streamSid}] No se encontr√≥ streamData para conectar OpenAI Realtime`);
      return;
    }

    try {
      logger.info(`ü§ñ [${streamSid}] Inicializando OpenAI Realtime Service con configuraci√≥n del cliente`);
      
      // Preparar configuraci√≥n del cliente
      const clientConfig = {
        companyName: streamData.client?.companyName || 'la empresa',
        companyDescription: streamData.client?.companyDescription || '',
        industry: streamData.client?.industry || '',
        ...streamData.client // Incluir toda la configuraci√≥n disponible
      };
      
      // Obtener contexto de memoria del llamante si existe
      const callerMemoryService = require('../services/callerMemoryService');
      let memoryContext = '';
      if (streamData.callerMemory) {
        memoryContext = callerMemoryService.getMemoryContext(streamData.callerMemory);
        logger.info(`üß† [${streamSid}] Contexto de memoria obtenido: ${memoryContext.length} caracteres`);
      }

      // Inicializar conexi√≥n OpenAI Realtime con callback de despedida
      await this.openaiRealtimeService.initializeConnection(streamSid, clientConfig, memoryContext);
      logger.info(`‚úÖ [${streamSid}] OpenAI Realtime Service inicializado correctamente`);
      
      // Configurar callback de despedida para colgar llamada autom√°ticamente
      const connectionData = this.openaiRealtimeService.activeConnections.get(streamSid);
      if (connectionData) {
        connectionData.onFarewell = () => {
          logger.info(`üìû [${streamSid}] Ejecutando cierre de llamada por despedida`);
          const streamData = this.activeStreams.get(streamSid);
          if (streamData?.twilioWs && streamData.twilioWs.readyState === 1) {
            logger.info(`üîå [${streamSid}] Cerrando WebSocket de Twilio`);
            streamData.twilioWs.close(1000, 'Farewell completed');
          }
        };
      }

    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error cr√≠tico inicializando OpenAI Realtime: ${error.message}`, error.stack);
      
      // üîß MANEJO ROBUSTO: El sistema puede funcionar sin OpenAI Realtime (solo saludo inicial)
      logger.warn(`‚ö†Ô∏è [${streamSid}] Sistema continuar√° solo con saludo inicial - conversaci√≥n inteligente deshabilitada`);
      
      // Marcar que OpenAI fall√≥ para el futuro
      const streamData = this.activeStreams.get(streamSid);
      if (streamData) {
        streamData.openaiRealtimeFailed = true;
      }
    }
  }

  /**
   * Maneja conexi√≥n WebSocket establecida
   */
  handleConnection(ws) {
    logger.info(`üîå NUEVA CONEXI√ìN WEBSOCKET ESTABLECIDA`);
    
    ws.connectionId = `conn_${Date.now()}_${Math.random().toString(36).substr(2, 8)}`;
    logger.info(`üîå Connection ID asignado: ${ws.connectionId}`);
    
    ws.on('message', async (message) => {
      try {
        const data = JSON.parse(message.toString());
        logger.info(`üì® [${ws.connectionId}] Mensaje recibido: ${data.event}`);
        await this.handleMessage(ws, data);
      } catch (error) {
        logger.error(`‚ùå Error procesando mensaje: ${error.message}`);
      }
    });

    ws.on('error', (error) => {
      logger.error(`‚ùå Error en WebSocket ${ws.connectionId}: ${error.message}`);
    });

    ws.on('close', async () => {
      logger.info(`üîå Conexi√≥n WebSocket cerrada: ${ws.connectionId}`);
      await this.cleanup(ws.connectionId);
    });
  }

  /**
   * Maneja mensajes WebSocket entrantes
   * @param {WebSocket} ws - Conexi√≥n WebSocket
   * @param {Object} data - Datos del mensaje
   */
  async handleMessage(ws, data) {
    const { event } = data;
    
    switch (event) {
      case 'connected':
        this.handleConnected(ws, data);
        break;
      case 'start':
        this.handleStart(ws, data);
        break;
      case 'media':
        await this.handleMediaEvent(ws, data);
        break;
      case 'mark':
        await this.handleMark(data);
        break;
      case 'stop':
        await this.handleStop(ws, data);
        break;
      default:
        logger.warn(`‚ö†Ô∏è Evento WebSocket no reconocido: ${event}`);
    }
  }

  /**
   * Maneja evento 'connected' de Twilio
   */
  handleConnected(ws, data) {
    logger.info(`üîå STREAM CONECTADO: ${JSON.stringify(data)}`);
    logger.info(`üîå [${ws.connectionId}] Twilio Stream conectado exitosamente`);

    // IMPORTANTE: El evento 'connected' NO incluye streamSid, por lo que necesitamos usar un ID temporal
    // Usaremos el connectionId como ID temporal hasta que llegue el 'start' con streamSid real
    const tempId = ws.connectionId; // Usar connectionId como ID temporal

    // Obtener datos del cliente usando la informaci√≥n de la llamada
    // Nota: En 'connected', no tenemos streamSid, pero tenemos callSid en el TwiML
    // Sin embargo, para simplicidad, intentaremos obtener el cliente si est√° disponible
    // Si no, lo haremos en 'start' cuando tengamos streamSid
    this.activeStreams.set(tempId, {
      callSid: data.callSid || 'unknown', // Si callSid est√° disponible en 'connected'
      state: 'connected',
      startTime: Date.now(),
      lastActivity: Date.now(),
      tempId: tempId // Marcar como temporal
    });

    logger.info(`üîÑ [${tempId}] Stream registrado con ID temporal (esperando streamSid en 'start')`);
  }

  /**
   * Maneja evento 'mark' de Twilio - CR√çTICO para activar transcripci√≥n
   */
  async handleMark(data) {
    const streamSid = data.streamSid;
    const markName = data.mark?.name;

    // üîß MEJORA: Validar que tenemos streamSid
    if (!streamSid) {
      logger.warn(`‚ö†Ô∏è Mensaje mark sin streamSid recibido - IGNORANDO. Datos: ${JSON.stringify(data)}`);
      return;
    }

    if (!markName) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Mensaje mark sin nombre recibido - IGNORANDO`);
      return;
    }

    logger.info(`üéØ [${streamSid}] Marca recibida: ${markName}`);
    // logger.info(`üîç [${streamSid}] DEBUG handleMark: markData = ${JSON.stringify(data.mark)}`);

    // Verificar si tenemos una acci√≥n pendiente para esta marca
    if (!this.pendingMarks) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] pendingMarks no inicializado - IGNORANDO marca ${markName}`);
      logger.error(`üîç DEBUG: pendingMarks is undefined! Checking constructor initialization.`);
      return;
    }

    if (!this.pendingMarks.has(markName)) {
      // logger.warn(`‚ö†Ô∏è [${streamSid}] Marca ${markName} no esperada (no est√° en pendingMarks) - IGNORANDO. pendingMarks entries: ${Array.from(this.pendingMarks.keys()).join(', ')}`);
      return;
    }

    const markData = this.pendingMarks.get(markName);
    logger.info(`üöÄ [${streamSid}] Ejecutando acci√≥n para marca ${markName}: ${markData.action}`);
    // logger.info(`üîç [${streamSid}] DEBUG handleMark: pendingMarks has ${this.pendingMarks.size} entries`);

    // Ejecutar la acci√≥n correspondiente
    switch (markData.action) {
      case 'activate_transcription':
        logger.info(`üé§ [${streamSid}] ACTIVANDO transcripci√≥n tras saludo (marca: ${markName})`);
        
        // ‚úÖ OpenAI ya est√° inicializado desde antes del saludo
        // Solo activar transcripci√≥n
        this.transcriptionActive.set(streamSid, true);
        const streamData = this.activeStreams.get(streamSid);
        if (streamData) {
          streamData.state = 'listening';
          streamData.greetingCompletedAt = Date.now();
        }
        
        // Desactivar echo blanking
        this.deactivateEchoBlanking(streamSid);
        
        logger.info(`‚úÖ [${streamSid}] Transcripci√≥n activada - usuario puede hablar`);
        break;
      case 'deactivate_echo_blanking':
        logger.info(`‚ö° [${streamSid}] DESACTIVANDO echo blanking tras completar respuesta (marca: ${markName})`);
        this.deactivateEchoBlanking(streamSid);
        this.responseInProgress.delete(streamSid);
        
        // üé§ REACTIVAR VAD de OpenAI - usuario puede interrumpir ahora
        const streamDataForVAD = this.activeStreams.get(streamSid);
        if (streamDataForVAD?.openAiService) {
          await streamDataForVAD.openAiService.enableVAD(streamSid);
        }
        
        logger.info(`‚úÖ [${streamSid}] Echo blanking desactivado - usuario puede hablar de nuevo`);
        break;
      default:
        logger.warn(`‚ö†Ô∏è [${streamSid}] Acci√≥n desconocida para marca ${markName}: ${markData.action} - IGNORANDO`);
    }

    // C√ìDIGO OFICIAL: Manejar marks del sistema OpenAI Realtime (responsePart)
    if (markName === 'responsePart') {
      logger.debug(`üìç [${streamSid}] Procesando marca responsePart del c√≥digo oficial`);
      // Notificar al OpenAI Realtime Service que la marca fue procesada
      this.openaiRealtimeService.processMarkCompletion(streamSid, markName);
      logger.debug(`‚úÖ [${streamSid}] Marca responsePart procesada y removida del queue`);
    }

    // Limpiar la marca procesada
    this.pendingMarks.delete(markName);
    logger.info(`üßπ [${streamSid}] Marca ${markName} procesada y limpiada`);
  }

  /**
   * Activa la transcripci√≥n despu√©s de que el audio termine (llamado por handleMark)
   */
  activateTranscriptionAfterAudio(streamSid) {
    logger.info(`üé§ [${streamSid}] ACTIVANDO transcripci√≥n tras saludo (marca: greeting_end)`);
    
    // Desactivar echo blanking
    this.deactivateEchoBlanking(streamSid);
    
    // Activar transcripci√≥n
    this.transcriptionActive.set(streamSid, true);
    
    const streamData = this.activeStreams.get(streamSid);
    if (streamData) {
      streamData.state = 'listening';
      streamData.greetingCompletedAt = Date.now();
    }
    
    // ‚úÖ NO REINICIALIZAR OPENAI - La conexi√≥n ya existe desde el saludo
    logger.info(`‚úÖ [${streamSid}] OpenAI inicializado y transcripci√≥n activada - usuario puede hablar`);
  }

  /**
   * Obtener datos del cliente para un stream usando callSid
   * @param {string} streamSid - Stream SID
   * @param {string} callSid - Call SID
   */
  async getClientForStream(streamSid, callSid) {
    try {
      logger.info(`üîç [${streamSid}] Obteniendo cliente para callSid: ${callSid}`);

      // üîß OPTIMIZAR: Verificar cache primero para evitar consultas duplicadas
      if (this.clientCache.has(callSid)) {
        const cachedClient = this.clientCache.get(callSid);
        logger.info(`‚ö° [${streamSid}] Cliente obtenido de cache: ${cachedClient.name || cachedClient.id}`);

        // Actualizar streamData con el cliente del cache
        const streamData = this.activeStreams.get(streamSid);
        if (streamData) {
          streamData.client = cachedClient;
        }
        return cachedClient;
      }

      // Buscar el cliente en la base de datos usando callSid
      // Nota: En producci√≥n, callSid puede no estar directamente en la DB, pero lo intentamos
      const client = await this.prisma.client.findFirst({
        where: {
          // Si tienes una relaci√≥n con llamadas, √∫sala aqu√≠
          // Por ahora, asumimos que callSid no est√° en la DB, as√≠ que usamos un cliente por defecto
          // O implementa la l√≥gica real si tienes callSid en tu esquema
        }
      });

      if (client) {
        // Cachear el cliente encontrado
        this.clientCache.set(callSid, client);

        // Actualizar streamData con el cliente
        const streamData = this.activeStreams.get(streamSid);
        if (streamData) {
          streamData.client = client;
          logger.info(`‚úÖ [${streamSid}] Cliente obtenido: ${client.name || client.id}`);
        }
      } else {
        logger.warn(`‚ö†Ô∏è [${streamSid}] No se encontr√≥ cliente para callSid: ${callSid} - usando cliente por defecto`);
        // Usar cliente por defecto o manejar error
        // Por simplicidad, asumir cliente ID 1 como antes
        const defaultClient = await this.prisma.client.findUnique({
          where: { id: 1 }
        });
        if (defaultClient) {
          // Cachear el cliente por defecto
          this.clientCache.set(callSid, defaultClient);

          const streamData = this.activeStreams.get(streamSid);
          if (streamData) {
            streamData.client = defaultClient;
            logger.info(`‚úÖ [${streamSid}] Cliente por defecto obtenido: ${defaultClient.name}`);
          }
        } else {
          logger.error(`‚ùå [${streamSid}] No se pudo obtener cliente por defecto`);
        }
      }
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error obteniendo cliente: ${error.message}`);
      throw error;
    }
  }

  /**
   * Maneja evento 'start' de Twilio Stream
   */
  handleStart(ws, data) {
    const streamSid = data.start?.streamSid;
    const callSid = data.start?.callSid;

    if (!streamSid) {
      logger.error('‚ùå No se recibi√≥ streamSid en evento start');
      return;
    }

    // üîí VERIFICACI√ìN CR√çTICA: Evitar procesamiento duplicado de streamSid
    if (this.processedStreamSids.has(streamSid)) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] StreamSid ya procesado anteriormente - IGNORANDO DUPLICADO`);
      return;
    }

    // üìä INCREMENTAR M√âTRICAS
    this.updateMetrics('start');
    
    logger.info(`üéµ [${streamSid}] Stream iniciado para llamada ${callSid}`);

    // Buscar el stream temporal usando connectionId
    const tempId = ws.connectionId;
    const tempStreamData = this.activeStreams.get(tempId);

    if (tempStreamData) {
      // Migrar del ID temporal al streamSid real
      this.activeStreams.set(streamSid, {
        ...tempStreamData,
        streamSid: streamSid,
        callSid: callSid,
        state: 'connected',
        startTime: Date.now(),
        lastActivity: Date.now(),
        greetingSent: false, // üîß EXPL√çCITO: Inicializar como false
        twilioWs: ws // NUEVO: Almacenar referencia WebSocket para OpenAI Realtime
      });

      // Eliminar el registro temporal
      this.activeStreams.delete(tempId);

      logger.info(`üîÑ [${streamSid}] Migrado de ID temporal ${tempId} a streamSid real`);
    } else {
      // Si no hay stream temporal, inicializar directamente
      this.activeStreams.set(streamSid, {
        callSid,
        streamSid,
        state: 'connected',
        startTime: Date.now(),
        lastActivity: Date.now(),
        greetingSent: false, // üîß EXPL√çCITO: Inicializar como false
        twilioWs: ws // NUEVO: Almacenar referencia WebSocket para OpenAI Realtime
      });
    }

    // ‚úÖ Marcar streamSid como procesado para evitar duplicados
    this.processedStreamSids.add(streamSid);

    // VERIFICACI√ìN CR√çTICA: Solo enviar saludo si no se ha enviado ya
    const existingStreamData = this.activeStreams.get(streamSid);
    if (existingStreamData?.greetingSent) {
      logger.info(`‚ö†Ô∏è [${streamSid}] Saludo ya enviado (greetingSent=true), omitiendo`);
      return;
    }

    logger.info(`üîç [${streamSid}] greetingSent status: ${existingStreamData?.greetingSent}`);

    // Inicializar sistemas necesarios
    // REMOVIDO: initializeSpeechDetection (VAD obsoleto - ahora usa OpenAI server VAD)
    this.initializeEchoBlanking(streamSid);

    // Obtener cliente, memoria del llamante y enviar saludo UNA SOLA VEZ
    this.getClientForStream(streamSid, callSid).then(async () => {
      // Obtener n√∫mero del llamante desde customParameters
      let streamData = this.activeStreams.get(streamSid);
      
      // üîç DEBUG: Log completo de customParameters
      logger.info(`üîç [${streamSid}] customParameters recibidos: ${JSON.stringify(data.start?.customParameters)}`);
      
      const callerPhone = data.start?.customParameters?.From || data.start?.customParameters?.from;
      
      logger.info(`üìû [${streamSid}] N√∫mero del llamante: ${callerPhone || 'NO DISPONIBLE'}`);
      logger.info(`üè¢ [${streamSid}] Cliente ID: ${streamData?.client?.id || 'NO DISPONIBLE'}`);
      
      // Obtener o crear memoria del llamante
      logger.info(`üîç [${streamSid}] DEBUG MEMORIA - clientId: ${streamData?.client?.id}, phone: ${callerPhone}`);
      
      if (streamData?.client?.id && callerPhone) {
        logger.info(`üß† [${streamSid}] Obteniendo memoria para ${callerPhone}`);
        const memory = await callerMemoryService.getOrCreateCallerMemory(
          streamData.client.id,
          callerPhone
        );
        
        if (memory) {
          // ‚ö†Ô∏è CR√çTICO: Actualizar directamente en activeStreams para que persista
          const currentStreamData = this.activeStreams.get(streamSid);
          if (currentStreamData) {
            currentStreamData.callerMemory = memory;
            logger.info(`‚úÖ [${streamSid}] Memoria cargada: ${memory.callCount} llamadas previas`);
            logger.info(`üìã [${streamSid}] Memoria ID: ${memory.id}, Nombre: ${memory.callerName || 'N/A'}, Empresa: ${memory.callerCompany || 'N/A'}`);
          }
        } else {
          logger.warn(`‚ö†Ô∏è [${streamSid}] No se pudo crear/obtener memoria`);
        }
      } else {
        logger.warn(`‚ö†Ô∏è [${streamSid}] No se puede crear memoria - clientId: ${streamData?.client?.id}, phone: ${callerPhone}`);
      }
      
      // Verificar de nuevo antes de enviar (doble verificaci√≥n)
      streamData = this.activeStreams.get(streamSid);
      
      // üîç VERIFICAR que la memoria persiste despu√©s de reasignar streamData
      logger.info(`üîç [${streamSid}] Memoria despu√©s de reasignaci√≥n - existe: ${!!streamData?.callerMemory}, callCount: ${streamData?.callerMemory?.callCount || 'N/A'}`);
      
      if (streamData?.greetingSent) {
        logger.info(`‚ö†Ô∏è [${streamSid}] Saludo ya enviado durante getClientForStream (greetingSent=true), omitiendo`);
        return;
      }

      logger.info(`üîç [${streamSid}] Enviando saludo despu√©s de getClientForStream - greetingSent: ${streamData?.greetingSent}`);

      // ‚è≥ ESPERAR a que se complete la carga de memoria antes de enviar saludo
      await this.sendInitialGreeting(ws, { streamSid, callSid }).catch(error => {
        logger.error(`‚ùå [${streamSid}] Error en saludo inicial: ${error.message}`);
      });
    }).catch(error => {
      logger.error(`‚ùå [${streamSid}] Error obteniendo cliente: ${error.message}`);
    });

    // Inicializar buffers y estados
    this.audioBuffers.set(streamSid, []);
    this.audioBuffer.set(streamSid, []);
    this.transcriptionActive.set(streamSid, false);
    this.responseInProgress.set(streamSid, false);
  }

  /**
   * Maneja evento 'stop' de Twilio Stream
   */
  async handleStop(ws, data) {
    const streamSid = data.stop?.streamSid;

    if (streamSid) {
      logger.info(`üõë [${streamSid}] Stream detenido`);
      await this.cleanup(streamSid);
    }
  }

  /**
   * Limpia recursos asociados a un stream
   */
  async cleanup(streamSid) {
    // üìä CALCULAR DURACI√ìN DE LLAMADA PARA M√âTRICAS
    const streamData = this.activeStreams.get(streamSid);
    const callDuration = streamData ? Date.now() - streamData.startTime : 0;
    const companyName = streamData?.client?.companyName || 'UNKNOWN';
    const callSid = streamData?.callSid || 'NO-SID';
    
    // üîç CORRELATION ID PARA DEBUGGING
    const correlationId = this.generateCorrelationId(streamSid, callSid, companyName);
    
    logger.info(`üßπ [${correlationId}] Iniciando limpieza de recursos (duration: ${callDuration}ms)`);

    // üìä ACTUALIZAR M√âTRICAS DE FINALIZACI√ìN
    if (callDuration > 0) {
      this.updateMetrics('success', callDuration);
    }

    // üß† ACTUALIZAR MEMORIA DEL LLAMANTE
    if (streamData?.callerMemory) {
      try {
        // Actualizar informaci√≥n del llamante si se obtuvo durante la llamada
        const updates = {};
        if (streamData.callerName) updates.callerName = streamData.callerName;
        if (streamData.callerCompany) updates.callerCompany = streamData.callerCompany;
        
        if (Object.keys(updates).length > 0) {
          await callerMemoryService.updateCallerInfo(streamData.callerMemory.id, updates);
          logger.info(`‚úÖ [${correlationId}] Informaci√≥n del llamante actualizada en memoria`);
        }
        
        // Obtener historial de conversaci√≥n de OpenAI Realtime
        const conversationHistory = await this.openaiRealtimeService.getConversationHistory(streamSid);
        
        logger.info(`üîç [${correlationId}] Historial obtenido de OpenAI:`);
        logger.info(`  - Summary: ${conversationHistory?.summary || 'N/A'}`);
        logger.info(`  - Topics: ${conversationHistory?.topics?.join(', ') || 'N/A'}`);
        logger.info(`  - Transcript length: ${conversationHistory?.transcript?.length || 0} chars`);
        
        // Crear resumen de conversaci√≥n con los detalles reales
        const conversationSummary = {
          summary: conversationHistory?.summary || `Llamada de ${Math.round(callDuration / 1000)}s`,
          duration: Math.round(callDuration / 1000),
          topics: conversationHistory?.topics || [],
          fullTranscript: conversationHistory?.transcript || '' // Guardar transcripci√≥n completa
        };
        
        logger.info(`üíæ [${correlationId}] Guardando en DB: ${JSON.stringify({
          summary: conversationSummary.summary.substring(0, 100),
          topics: conversationSummary.topics,
          transcriptLength: conversationSummary.fullTranscript.length
        })}`);
        
        await callerMemoryService.addConversationToHistory(
          streamData.callerMemory.id,
          conversationSummary
        );
        logger.info(`‚úÖ [${correlationId}] Conversaci√≥n guardada en memoria ID: ${streamData.callerMemory.id}`);
      } catch (error) {
        logger.error(`‚ùå [${correlationId}] Error actualizando memoria: ${error.message}`);
      }
    }

    // NUEVO: Cerrar conexi√≥n OpenAI Realtime
    if (this.openaiRealtimeService.isConnectionActive(streamSid)) {
      logger.info(`ü§ñ [${correlationId}] Cerrando conexi√≥n OpenAI Realtime`);
      await this.openaiRealtimeService.closeConnection(streamSid);
    }

    // üîß NUEVO: Limpiar clientCache entries relacionadas ANTES de borrar streamData
    if (streamData?.callSid) {
      this.clientCache.delete(streamData.callSid);
      logger.debug(`üßπ [${correlationId}] ClientCache limpiado para callSid: ${streamData.callSid}`);
    }

    // Limpiar todos los mapas y recursos
    this.activeStreams.delete(streamSid);
    this.audioBuffers.delete(streamSid);
    this.transcriptionActive.delete(streamSid);
    this.responseInProgress.delete(streamSid);
    this.silenceStartTime.delete(streamSid);
    this.lastResponseTime.delete(streamSid);
    // REMOVIDO: speechDetection y energySamples (VAD obsoleto)
    this.echoBlanking.delete(streamSid);
    this.pendingMarks.delete(streamSid);

    // üîß CR√çTICO: Limpiar processedStreamSids para evitar memory leaks
    this.processedStreamSids.delete(streamSid);

    logger.info(`‚úÖ [${streamSid}] Recursos limpiados correctamente`);
  }

  /**
   * üîç GENERAR CORRELATION ID PARA DEBUGGING
   * @param {string} streamSid - Stream SID
   * @param {string} callSid - Call SID  
   * @param {string} companyName - Nombre de empresa
   * @returns {string} Correlation ID √∫nico
   */
  generateCorrelationId(streamSid, callSid, companyName = 'UNKNOWN') {
    const shortStreamSid = streamSid ? streamSid.slice(-8) : 'NO-SID';
    const shortCallSid = callSid ? callSid.slice(-8) : 'NO-CALL';
    const company = companyName.substring(0, 10).toUpperCase();
    return `${company}|${shortStreamSid}|${shortCallSid}`;
  }

  /**
   * üìä OBTENER M√âTRICAS PARA MONITOREO
   * @returns {Object} M√©tricas actuales del sistema
   */
  getMetrics() {
    return {
      ...this.metrics,
      activeStreams: this.activeStreams.size,
      pendingMarks: this.pendingMarks.size,
      clientCacheSize: this.clientCache.size,
      uptime: Date.now() - this.metrics.lastReset
    };
  }

  /**
   * üìä ACTUALIZAR M√âTRICAS DE LLAMADA
   * @param {string} type - Tipo de evento (start|success|fail)
   * @param {number} duration - Duraci√≥n en ms (opcional)
   */
  updateMetrics(type, duration = 0) {
    switch (type) {
      case 'start':
        this.metrics.totalCalls++;
        this.metrics.activeCalls++;
        break;
      case 'success':
        this.metrics.successfulCalls++;
        this.metrics.activeCalls--;
        if (duration > 0) {
          const avgDuration = this.metrics.averageCallDuration;
          const totalSuccessful = this.metrics.successfulCalls;
          this.metrics.averageCallDuration = ((avgDuration * (totalSuccessful - 1)) + duration) / totalSuccessful;
        }
        break;
      case 'fail':
        this.metrics.failedCalls++;
        this.metrics.activeCalls--;
        break;
      case 'openai_fail':
        this.metrics.openaiFailures++;
        break;
      case 'azure_fail':
        this.metrics.azureFailures++;
        break;
    }
  }

  /**
   * Maps a user-friendly voice name to a valid Azure TTS voice identifier
   * @param {string} voiceId - User-friendly voice name
   * @param {string} language - Language code (e.g., 'es-ES', 'en-US')
   * @returns {string} Valid Azure TTS voice identifier
   */
  mapVoiceToAzure(voiceId, language = 'es-ES') {
    // Siempre usar Isidora Multiling√ºe para todos los usuarios (configuraci√≥n optimizada)
    // logger.info(`üéµ Using Isidora Multilingual voice for all users: ${this.defaultVoice}`);
    return this.defaultVoice;
  }

  /**
   * Humanizar texto con SSML para que Isidora Multiling√ºe suene m√°s natural
   * @param {string} text - Texto a humanizar
   * @param {string} style - Estilo SSML: 'chat', 'empathetic', 'friendly', 'calm'
   * @returns {string} Texto con SSML aplicado (solo contenido interno)
   */
  humanizeTextWithSSML(text, style = 'chat') {
    // Limpiar texto de posibles caracteres problem√°ticos
    const cleanText = text.replace(/[<>&"']/g, (match) => {
      const entities = { '<': '&lt;', '>': '&gt;', '&': '&amp;', '"': '&quot;', "'": '&apos;' };
      return entities[match];
    });

    // Solo devolver el contenido SSML interno (sin <speak> wrapper)
    // El servicio TTS ya agrega el wrapper completo
    // Estilos optimizados para Isidora Multiling√ºe - velocidad 1.1 en todos
    const styleSettings = {
      'chat': { rate: '1.1', pitch: '-2%', volume: '90%', breakTime: '400ms' },
      'empathetic': { rate: '1.1', pitch: '-3%', volume: '85%', breakTime: '500ms' },
      'friendly': { rate: '1.1', pitch: '+1%', volume: '95%', breakTime: '250ms' },
      'calm': { rate: '1.1', pitch: '-4%', volume: '80%', breakTime: '600ms' },
      'cheerful': { rate: '1.1', pitch: '+2%', volume: '95%', breakTime: '300ms' }
    };
    
    const settings = styleSettings[style] || styleSettings['chat'];
    
    const ssmlContent = `
          <mstts:express-as style="${style}">
            <prosody rate="${settings.rate}" pitch="${settings.pitch}" volume="${settings.volume}">
              ${cleanText.replace(/\./g, `.<break time="${settings.breakTime}"/>`)}
            </prosody>
          </mstts:express-as>
    `.trim();

    logger.info(`üé≠ SSML humanizado aplicado: ${ssmlContent.substring(0, 100)}...`);
    return ssmlContent;
  }  

  /**
   * Stream conectado - SOLO registrar conexi√≥n
   */

  // Generar saludo inicial - SOLO UNA VEZ POR STREAM
  async sendInitialGreeting(ws, { streamSid, callSid }) {
    logger.info(`üîç [${streamSid}] INICIANDO sendInitialGreeting`);
    const streamData = this.activeStreams.get(streamSid);
    if (!streamData?.client) {
      logger.error(`‚ùå [${streamSid}] Sin configuraci√≥n de cliente`);
      return;
    }

    // Marcar como saludo enviado para evitar duplicados
    streamData.greetingSent = true;
    // logger.info(`üîç [${streamSid}] Marcando saludo como enviado`);

    const clientConfigData = await this.prisma.client.findUnique({
      where: { id: parseInt(streamData.client.id) },
      select: {
        callConfig: true
      }
    });

    const greeting = clientConfigData.callConfig?.greeting;
    logger.info(`üîä [${streamSid}] Using greeting from DB: "${greeting}"`);
    
    // Get voice configuration and map to valid Azure voice
    const rawVoiceId = streamData.client.callConfig?.voiceId || 
                      clientConfigData.callConfig?.voiceId || 
                      'isidora';
    const language = streamData.client.callConfig?.language || 
                    clientConfigData.callConfig?.language || 
                    'es-ES';
    
    // DEBUG: Log complete callConfig structure (solo si hay problemas)
    // logger.info(`üîç [${streamSid}] Complete callConfig from streamData: ${JSON.stringify(streamData.client.callConfig, null, 2)}`);
    // logger.info(`üîç [${streamSid}] Complete callConfig from DB: ${JSON.stringify(clientConfigData.callConfig, null, 2)}`);
    
    const voiceId = this.mapVoiceToAzure(rawVoiceId, language);
    
    logger.info(`üéµ [${streamSid}] Raw voice from DB: "${rawVoiceId}"`);
    logger.info(`üéµ [${streamSid}] Mapped Azure voice: "${voiceId}"`);
    logger.info(`üåç [${streamSid}] Language: "${language}"`);
  
    logger.info(`üîä [${streamSid}] Generando saludo: "${greeting?.substring(0, 50)}..."`);

    // Verificar longitud m√≠nima (10 caracteres)
    if (!greeting || greeting.length < 10) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Saludo muy corto o vac√≠o: "${greeting}" - usando fallback`);
      await this.sendExtendedGreeting(ws, streamSid, streamData.client);
      return;
    }

    // üéØ ESTRATEGIA: Inicializar OpenAI PRIMERO, generar saludo, enviarlo, y LUEGO activar transcripci√≥n
    // Esto evita que OpenAI detecte su propio saludo como voz del usuario
    
    // ‚úÖ SOLUCI√ìN: Inicializar OpenAI y enviar mensaje para que genere el saludo
    
    try {
      // 1. Obtener contexto de memoria del llamante si existe
      const callerMemoryService = require('../services/callerMemoryService');
      let memoryContext = '';
      
      logger.info(`üîç [${streamSid}] DEBUG - streamData.callerMemory existe: ${!!streamData.callerMemory}`);
      if (streamData.callerMemory) {
        logger.info(`üîç [${streamSid}] DEBUG - callerMemory.callCount: ${streamData.callerMemory.callCount}`);
        memoryContext = callerMemoryService.getMemoryContext(streamData.callerMemory);
        logger.info(`üß† [${streamSid}] Contexto de memoria obtenido: ${memoryContext.length} caracteres`);
        logger.info(`üìù [${streamSid}] Primeras 200 chars del contexto: ${memoryContext.substring(0, 200)}`);
      } else {
        logger.warn(`‚ö†Ô∏è [${streamSid}] NO HAY MEMORIA DEL LLAMANTE - se enviar√° sin contexto hist√≥rico`);
      }
      
      // 2. Inicializar OpenAI Realtime con contexto de memoria
      logger.info(`ü§ñ [${streamSid}] Inicializando OpenAI Realtime...`);
      await this.openaiRealtimeService.initializeConnection(streamSid, streamData.client, memoryContext);
      logger.info(`‚úÖ [${streamSid}] OpenAI Realtime inicializado`);
      
      // 2. Enviar mensaje para activar el saludo
      logger.info(`üì§ [${streamSid}] Enviando trigger para generar saludo: "${greeting}"`);
      await this.openaiRealtimeService.sendGreetingTrigger(streamSid, greeting);
      
      // 3. Activar transcripci√≥n para que el usuario pueda hablar despu√©s
      this.transcriptionActive.set(streamSid, true);
      streamData.state = 'listening';
      
      logger.info(`‚úÖ [${streamSid}] Sistema listo - OpenAI generar√° saludo`);
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error inicializando OpenAI: ${error.message}`);
    }
    
    logger.info(`üîç [${streamSid}] FINALIZANDO sendInitialGreeting`);
  }

  async sendExtendedGreeting(ws, streamSid, clientConfigData) {
    const fallbackGreeting = "Hola, bienvenido. ¬øEn qu√© puedo ayudarte?";
    
    logger.info(`üîä [${streamSid}] Generando saludo extendido de fallback con OpenAI`);
    
    try {
      // Obtener contexto de memoria del llamante si existe
      const streamData = this.activeStreams.get(streamSid);
      const callerMemoryService = require('../services/callerMemoryService');
      let memoryContext = '';
      if (streamData?.callerMemory) {
        memoryContext = callerMemoryService.getMemoryContext(streamData.callerMemory);
        logger.info(`üß† [${streamSid}] Contexto de memoria obtenido: ${memoryContext.length} caracteres`);
      }
      
      // 1. Inicializar OpenAI Realtime
      logger.info(`ü§ñ [${streamSid}] Inicializando OpenAI Realtime para saludo extendido...`);
      await this.openaiRealtimeService.initializeConnection(streamSid, clientConfigData, memoryContext);
      logger.info(`‚úÖ [${streamSid}] OpenAI Realtime inicializado`);
      
      // 2. Generar audio del saludo con OpenAI TTS
      logger.info(`üîä [${streamSid}] Generando saludo extendido con OpenAI TTS...`);
      const openaiTTSResult = await this.openaiRealtimeService.generateGreetingAudio(streamSid, fallbackGreeting);
      
      if (openaiTTSResult.success) {
        logger.info(`‚úÖ [${streamSid}] Audio de saludo extendido generado (${openaiTTSResult.audioBuffer.length} bytes)`);
        
        // ECHO BLANKING: Activar blanking antes de enviar audio del bot
        this.activateEchoBlanking(streamSid);
        
        // Usar sistema de marcas para activar transcripci√≥n
        const markId = `extended_greeting_end_${Date.now()}`;
        logger.info(`üéØ [${streamSid}] Enviando saludo extendido con marca: ${markId}`);
        
        // Registrar que esperamos esta marca para activar transcripci√≥n
        this.pendingMarks = this.pendingMarks || new Map();
        this.pendingMarks.set(markId, {
          streamSid: streamSid,
          action: 'activate_transcription',
          timestamp: Date.now()
        });
        
        // Enviar audio con marca al final
        await this.sendRawMulawToTwilioWithMark(ws, openaiTTSResult.audioBuffer, streamSid, markId);
        logger.info(`‚úÖ [${streamSid}] Saludo extendido enviado con marca ${markId}`);
      } else {
        throw new Error('OpenAI TTS failed for extended greeting');
      }
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error en saludo extendido: ${error.message}`);
      
      // Usar audio de fallback si TTS falla
      logger.warn(`‚ö†Ô∏è [${streamSid}] Usando audio de fallback para saludo extendido`);
      
      // Activar echo blanking durante fallback
      this.activateEchoBlanking(streamSid);
      
      // Usar sistema de marcas para activar transcripci√≥n despu√©s del fallback extendido
      const markId = `extended_fallback_end_${Date.now()}`;
      logger.info(`üéØ [${streamSid}] Enviando fallback extendido con marca: ${markId}`);
      
      // Registrar que esperamos esta marca para activar transcripci√≥n
      this.pendingMarks = this.pendingMarks || new Map();
      this.pendingMarks.set(markId, {
        streamSid: streamSid,
        action: 'activate_transcription',
        timestamp: Date.now()
      });
      
      // Enviar fallback con marca al final
      await this.sendRawMulawToTwilioWithMark(ws, this.fallbackAudio, streamSid, markId);
      logger.info(`‚úÖ [${streamSid}] Fallback del saludo extendido enviado con marca ${markId}`);

      // üîß CR√çTICO: Desactivar echo blanking despu√©s del audio de fallback
      setTimeout(() => {
        this.deactivateEchoBlanking(streamSid);
      }, 2000); // 2 segundos para el audio de fallback
    }
  }

  async sendRawMulawToTwilio(ws, mulawBuffer, streamSid) {
    const chunkSize = 160;
    let offset = 0;
    
    while (offset < mulawBuffer.length) {
      const chunk = mulawBuffer.subarray(offset, offset + chunkSize);
      const base64Chunk = chunk.toString('base64');
      
      ws.send(JSON.stringify({
        event: 'media',
        streamSid: streamSid,
        media: { payload: base64Chunk }
      }));
      
      offset += chunkSize;
    }
    
    logger.info(`‚úÖ [${streamSid}] Audio enviado: ${Math.ceil(mulawBuffer.length / chunkSize)} chunks`);
  }

  /**
   * Enviar audio con marca para detectar cuando termina la reproducci√≥n
   */
  async sendRawMulawToTwilioWithMark(ws, mulawBuffer, streamSid, markId) {
    logger.info(`üîä [${streamSid}] Enviando audio con marca: ${markId}`);
    logger.info(`üîä Tama√±o del buffer de audio: ${mulawBuffer.length} bytes`);
    
    // Ensure minimum audio length (1 second = 8000 bytes)
    if (mulawBuffer.length < 8000) {
      const padding = Buffer.alloc(8000 - mulawBuffer.length, 0xFF);
      mulawBuffer = Buffer.concat([mulawBuffer, padding]);
      logger.info(`üîä [${streamSid}] A√±adido padding de audio: ${padding.length} bytes`);
    }
    
    const chunkSize = 160;
    let offset = 0;
    let chunkCount = 0;
    
    logger.info(`üéµ [${streamSid}] Iniciando transmisi√≥n de audio con marca (${mulawBuffer.length} bytes)`);
    
    // Enviar todos los chunks de audio
    while (offset < mulawBuffer.length) {
      const chunk = mulawBuffer.subarray(offset, offset + chunkSize);
      const base64Chunk = chunk.toString('base64');
      
      ws.send(JSON.stringify({
        event: 'media',
        streamSid: streamSid,
        media: { payload: base64Chunk }
      }));
      
      chunkCount++;
      offset += chunkSize;
    }
    
    // CR√çTICO: Enviar marca DESPU√âS del audio para detectar cuando termina
    ws.send(JSON.stringify({
      event: 'mark',
      streamSid: streamSid,
      mark: { name: markId }
    }));
    
    logger.info(`‚úÖ [${streamSid}] Audio enviado (${chunkCount} chunks) + marca ${markId}`);
  }

  generateFallbackAudio() {
    // Crear audio de 2 segundos (8000 samples * 2 = 16000 bytes) en lugar de 160 bytes
    const buffer = Buffer.alloc(16000, 0xFF); // Silencio audible de 2 segundos
    return buffer;
  };

  // REMOVIDO: initializeSpeechDetection() - VAD obsoleto, ahora usa OpenAI server VAD

  /**
   * Inicializar sistema de echo blanking para un stream
   */
  initializeEchoBlanking(streamSid) {
    this.echoBlanking.set(streamSid, {
      active: false,
      endTime: 0,
      lastActivation: 0
    });
    logger.info(`üîá [${streamSid}] Echo blanking inicializado`);
  }

  /**
   * Activar Echo Blanking cuando el bot va a hablar
   */
  activateEchoBlanking(streamSid) {
    const echoBlanking = this.echoBlanking.get(streamSid);
    if (echoBlanking) {
      const now = Date.now();
      const duration = 2000; // 2 seconds echo blanking duration
      echoBlanking.active = true;
      echoBlanking.endTime = now + duration;
      logger.info(`üîá [${streamSid}] Echo Blanking ACTIVADO por ${duration}ms`);
    }
  }

  /**
   * Desactivar Echo Blanking manualmente
   */
  deactivateEchoBlanking(streamSid) {
    logger.info(`üîç [${streamSid}] INICIANDO deactivateEchoBlanking()`);
    
    const echoBlanking = this.echoBlanking.get(streamSid);
    logger.info(`üîç [${streamSid}] echoBlanking obtenido:`, echoBlanking);
    
    if (!echoBlanking) {
      logger.error(`‚ùå [${streamSid}] NO HAY echoBlanking en el mapa`);
      return;
    }
    
    logger.info(`üîç [${streamSid}] echoBlanking.active = ${echoBlanking.active}`);
    
    if (echoBlanking && echoBlanking.active) {
      logger.info(`üîç [${streamSid}] Desactivando echo blanking...`);
      echoBlanking.active = false;
      echoBlanking.endTime = 0;
      logger.info(`üîá [${streamSid}] Echo Blanking DESACTIVADO`);
      
      // Activar transcripci√≥n autom√°ticamente cuando se desactiva echo blanking
      const streamData = this.activeStreams.get(streamSid);
      logger.info(`üîç [${streamSid}] streamData obtenido:`, streamData ? 'EXISTS' : 'NULL');
      
      const currentTranscriptionState = this.transcriptionActive.get(streamSid);
      logger.info(`üîç [${streamSid}] Estado actual transcripci√≥n: ${currentTranscriptionState}`);
      
      if (streamData && !this.transcriptionActive.get(streamSid)) {
        logger.info(`üîç [${streamSid}] ACTIVANDO TRANSCRIPCI√ìN...`);
        this.transcriptionActive.set(streamSid, true);
        streamData.state = 'listening';
        streamData.greetingCompletedAt = Date.now();
        
        // Verificar que se activ√≥ correctamente
        const newTranscriptionState = this.transcriptionActive.get(streamSid);
        logger.info(`üöÄ [${streamSid}] Transcripci√≥n activada! Nuevo estado: ${newTranscriptionState}`);
        logger.info(`üöÄ [${streamSid}] streamData.state = ${streamData.state}`);
      } else {
        logger.warn(`‚ö†Ô∏è [${streamSid}] NO se activ√≥ transcripci√≥n:`);
        logger.warn(`‚ö†Ô∏è [${streamSid}] - streamData existe: ${!!streamData}`);
        logger.warn(`‚ö†Ô∏è [${streamSid}] - transcripci√≥n ya activa: ${this.transcriptionActive.get(streamSid)}`);
      }
    } else {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Echo blanking NO estaba activo o no existe`);
      logger.warn(`‚ö†Ô∏è [${streamSid}] - echoBlanking existe: ${!!echoBlanking}`);
      logger.warn(`‚ö†Ô∏è [${streamSid}] - echoBlanking.active: ${echoBlanking?.active}`);
    }
    
    logger.info(`üîç [${streamSid}] FINALIZANDO deactivateEchoBlanking()`);
  }

  /**
   * Verificar si Echo Blanking est√° activo
   */
  isEchoBlankingActive(streamSid) {
    const echoBlanking = this.echoBlanking.get(streamSid);
    if (!echoBlanking) return false;
    
    const now = Date.now();
    if (echoBlanking.active && now > echoBlanking.endTime) {
      echoBlanking.active = false;
      logger.info(`üîä [${streamSid}] Echo Blanking DESACTIVADO`);
    }
    
    return echoBlanking.active;
  }

  /**
   * Analizar calidad y caracter√≠sticas del buffer de audio
   */

  /**
   * H√çBRIDO: Manejar eventos de media con timestamps del c√≥digo oficial
   */
  async handleMediaEvent(ws, data) {
    const { streamSid } = data;
    const streamData = this.activeStreams.get(streamSid);
    
    // Solo procesar si transcripci√≥n est√° activa (despu√©s del saludo)
    if (!streamData || !this.transcriptionActive.get(streamSid)) {
      return;
    }

    // No procesar si echo blanking est√° activo (bot hablando)
    if (this.isEchoBlankingActive(streamSid)) {
      return;
    }

    const payload = data.media?.payload;
    const mediaTimestamp = data.media?.timestamp; // C√ìDIGO OFICIAL: Capturar timestamp
    const track = data.media?.track; // VALIDACI√ìN: Verificar track type
    
    // üîß VALIDACI√ìN: Solo procesar audio inbound seg√∫n est√°ndares Twilio
    if (!payload || track !== 'inbound') {
      logger.debug(`üîç [${streamSid}] Ignorando media: payload=${!!payload}, track=${track}`);
      return;
    }

    // üîß SEGURIDAD: Validar tama√±o de chunk seg√∫n l√≠mites Twilio (16KB m√°ximo)
    try {
      const audioBuffer = Buffer.from(payload, 'base64');
      if (audioBuffer.length > 16384) { // 16KB m√°ximo por chunk Twilio
        logger.warn(`‚ö†Ô∏è [${streamSid}] Chunk de audio demasiado grande: ${audioBuffer.length} bytes (m√°ximo: 16KB)`);
        return;
      }
    } catch (error) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] Error decodificando payload de audio: ${error.message}`);
      return;
    }

    // Verificar si OpenAI Realtime est√° conectado
    if (!this.openaiRealtimeService.isConnectionActive(streamSid)) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] OpenAI Realtime no est√° activo, ignorando audio`);
      return;
    }

    // C√ìDIGO OFICIAL: Log timestamps para debugging
    if (mediaTimestamp) {
      logger.debug(`‚è±Ô∏è [${streamSid}] Received media with timestamp: ${mediaTimestamp}ms`);
    }

    // Enviar audio a OpenAI Realtime con timestamp
    try {
      this.openaiRealtimeService.sendAudioToOpenAI(streamSid, payload, mediaTimestamp);
      logger.debug(`üéôÔ∏è [${streamSid}] Audio enviado a OpenAI Realtime`);
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error enviando audio a OpenAI: ${error.message}`);
    }
  }

  /**
   * C√ìDIGO OFICIAL: Handle clear audio event (interrupciones)
   * @param {Object} data - {streamSid}
   */
  handleClearAudio(data) {
    const { streamSid } = data;
    const streamData = this.activeStreams.get(streamSid);
    
    if (streamData && streamData.twilioWs) {
      logger.info(`üîÑ [${streamSid}] Enviando clear event a Twilio (interrupci√≥n)`);
      
      streamData.twilioWs.send(JSON.stringify({
        event: 'clear',
        streamSid: streamSid
      }));
    }
  }

  /**
   * C√ìDIGO OFICIAL: Handle send mark event
   * @param {Object} data - {streamSid, markName}
   */
  handleSendMark(data) {
    const { streamSid, markName } = data;
    const streamData = this.activeStreams.get(streamSid);
    
    if (streamData && streamData.twilioWs) {
      logger.debug(`üìç [${streamSid}] Enviando marca a Twilio: ${markName}`);
      
      streamData.twilioWs.send(JSON.stringify({
        event: 'mark',
        streamSid: streamSid,
        mark: { name: markName }
      }));
    }
  }

  /**
   * ‚úÖ NUEVO FLUJO SIMPLE: Procesar texto de OpenAI con Azure TTS (como saludo inicial)
   * @param {Object} data - {streamSid, text, timestamp}
   */
  async handleProcessTextWithAzure(data) {
    const { streamSid, text, timestamp } = data;
    const streamData = this.activeStreams.get(streamSid);
    
    if (!streamData) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] No hay streamData para procesar texto con Azure`);
      return;
    }

    logger.info(`üöÄ [${streamSid}] ‚úÖ PROCESANDO TEXTO con Azure TTS`);
    logger.info(`üìù [${streamSid}] Texto: "${text}"`);

    try {
      // ‚úÖ EXACTAMENTE como sendInitialGreeting: texto ‚Üí Azure TTS ‚Üí Twilio
      const ttsResult = await this.ttsService.generateSpeech(
        text, 
        this.defaultVoice, 
        'raw-8khz-8bit-mono-mulaw'
      );
      
      if (ttsResult.success) {
        logger.info(`‚úÖ [${streamSid}] Azure TTS completado (${ttsResult.audioBuffer.length} bytes)`);
        
        // Activar echo blanking para respuesta
        this.activateEchoBlanking(streamSid);
        
        // Enviar audio con marca (como saludo inicial)
        const markId = `response_${Date.now()}`;
        this.pendingMarks.set(markId, {
          streamSid: streamSid,
          action: 'deactivate_echo_blanking',
          timestamp: Date.now()
        });
        
        await this.sendRawMulawToTwilioWithMark(streamData.twilioWs, ttsResult.audioBuffer, streamSid, markId);
        logger.info(`‚úÖ [${streamSid}] Respuesta enviada con marca ${markId}`);
        
      } else {
        logger.error(`‚ùå [${streamSid}] Error en Azure TTS: ${ttsResult.error}`);
      }
      
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error procesando texto con Azure: ${error.message}`, error.stack);
      
      // En caso de error, desactivar echo blanking
      try {
        this.deactivateEchoBlanking(streamSid);
        logger.warn(`‚ö° [${streamSid}] Echo blanking desactivado tras error`);
      } catch (emergencyError) {
        logger.error(`üí• [${streamSid}] Error en desactivaci√≥n de emergencia: ${emergencyError.message}`);
      }
    }
  }

  /**
   * üöÄ AUDIO NATIVO: Manejar audio directo de OpenAI (sin Azure TTS)
   * @param {Object} data - {streamSid, audio, timestamp}
   */
  async handleAudioFromOpenAI(data) {
    const { streamSid, audio, timestamp } = data;
    const streamData = this.activeStreams.get(streamSid);
    
    if (!streamData || !streamData.twilioWs) {
      logger.warn(`‚ö†Ô∏è [${streamSid}] No hay conexi√≥n Twilio para audio de OpenAI`);
      return;
    }

    try {
      // Enviar audio mulaw directamente a Twilio (sin conversi√≥n)
      const mediaMessage = {
        event: 'media',
        streamSid: streamSid,
        media: {
          payload: audio  // Base64 mulaw de OpenAI
        }
      };

      streamData.twilioWs.send(JSON.stringify(mediaMessage));
      logger.debug(`üéµ [${streamSid}] Audio chunk enviado a Twilio (${audio.length} chars)`);
      
    } catch (error) {
      logger.error(`‚ùå [${streamSid}] Error enviando audio de OpenAI a Twilio: ${error.message}`);
    }
  }

  // üóëÔ∏è FUNCIONES OBSOLETAS ELIMINADAS: 
  // - handleProcessAudioWithAzure() 
  // - processAudioDeltaWithAzure()
  // 
  // RAZ√ìN: Solo usamos transcripci√≥n de OpenAI ‚Üí Azure TTS, no audio directo

}

module.exports = TwilioStreamHandler;
